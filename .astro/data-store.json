[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.16.6","content-config-digest","e2cbfa1c4b1425b4","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://your-domain.com\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":true,\"transformers\":[]},\"remarkPlugins\":[null],\"rehypePlugins\":[null],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","blog",["Map",11,12,76,77],"rope",{"id":11,"data":13,"body":20,"filePath":21,"digest":22,"rendered":23,"legacyId":75},{"title":14,"date":15,"tags":16,"excerpt":17,"cover":17,"status":18,"notionId":19},"RoPE 究竟是怎么计算的",["Date","2025-12-25T00:00:00.000Z"],[],"","published","2d322dca-4210-8074-95ce-ec86131a7787","---\n\n\n## 一、RoPE 到底改了什么？\n\n\nAttention 里我们原来用：\n\n- $Q, K, V$ 计算 $\\text{Attn}(Q, K, V)$\n\nRoPE 做的是把 **Q/K 先旋转**：\n\n- $Q' = \\text{RoPE}(Q, \\text{pos})$\n- $K' = \\text{RoPE}(K, \\text{pos})$\n- 然后用 $Q', K', V$ 做 attention\n\n---\n\n\n## 二、数学形式：2D 旋转（每两维一组）\n\n\n对每个 position $p$，对某一对维度 $(u,v)$ 做旋转：\n\n\n\n$$\n\\begin{bmatrix}\nx'_u \\\\\nx'_v\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\cos\\theta & -\\sin\\theta \\\\\n\\sin\\theta & \\cos\\theta\n\\end{bmatrix}\n\\begin{bmatrix}\nx_u \\\\\nx_v\n\\end{bmatrix}\n$$\n\n\n\n工程实现常写成统一模板：\n\n\n$⁡x_{\\text{rot}} = x \\cdot \\cos + \\text{rotate\\_half}(x)\\cdot \\sin$\n\n\n关键：\n\n- **`rotate_half`** **定义了“哪两维是一对”**\n- **`cos/sin`** **展开方式必须和这个配对一致**\n\n---\n\n\n## 三、两种 RoPE style：差别只在“怎么配对维度”\n\n\n假设 head_dim $D=8$，向量：$x=[x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7]$\n\n\n### 3.1 Adjacent-pair（相邻配对 / even-odd）\n\n\n配对方式：\n\n- $(0,1), (2,3), (4,5), (6,7)$\n\n对应的 `rotate_half`（相邻两两转）会把每对做 $(−y,x)$：\n\n- 输出：`[-x1, x0, -x3, x2, -x5, x4, -x7, x6]`\n\n### 3.2 NeoX-style（前后半配对 / split-half）\n\n\n配对方式：\n\n- $(0,4),(1,5),(2,6),(3,7)$\n\n对应的 `rotate_half`（前后半互换再取负）：\n\n- 输出：`[-x4, -x5, -x6, -x7, x0, x1, x2, x3]`\n\n---\n\n\n## 四、最容易踩坑的点：cos/sin 怎么“铺满到 D 维”\n\n\n先算基础的 ⁡$\\cos,\\sin$（每对一个频率），它们天然是长度 $D/2$ 的向量：\n\n- `cos_base = [c0 c1 c2 c3]`\n- `sin_base = [s0 s1 s2 s3]`\n\n### 4.1 NeoX-style 的展开（**拼两份**）\n\n\n因为配对是 `(i, i+D/2)`，所以要变成：\n\n- `cos_full = [c0 c1 c2 c3 c0 c1 c2 c3]`\n- `sin_full = [s0 s1 s2 s3 s0 s1 s2 s3]`\n\n### 4.2 Adjacent-pair 的展开（**每个重复两次**）\n\n\n因为配对是 `(2i, 2i+1)`，所以要变成：\n\n- `cos_full = [c0 c0 c1 c1 c2 c2 c3 c3]`\n- `sin_full = [s0 s0 s1 s1 s2 s2 s3 s3]`\n\n**结论（牢记）**：\n\n- NeoX-style：`rotate_half = cat([-x2, x1])` 必须配 `cos/sin = cat([cos, cos])`\n- Adjacent-pair：`rotate_half` 用 `::2 / 1::2` 必须配 `repeat_interleave(2)`\n\n不匹配通常会出现：\n\n> 不报 shape 错，但模型效果/困惑度明显崩掉\n\n---\n\n\n## 五、参考实现（布局：`[B, T, H, D]`）\n\n\n### 5.1 两种 rotate_half\n\n\n```python\nimport torch\n\ndef rotate_half_adjacent(x):  # x: [..., rotary_dim]\n    # pair: (0,1), (2,3), ...\n    x_even = x[..., ::2]\n    x_odd  = x[..., 1::2]\n    out = torch.stack((-x_odd, x_even), dim=-1)\n    return out.flatten(-2)  # restore last dim\n\ndef rotate_half_neox(x):  # x: [..., rotary_dim]\n    # pair: (0, D/2), (1, D/2+1), ...\n    half = x.shape[-1] // 2\n    x1, x2 = x[..., :half], x[..., half:]\n    return torch.cat((-x2, x1), dim=-1)\n```\n\n\n### 5.2 构建 cos/sin cache（核心：不同 style 的展开）\n\n\n```python\ndef build_rope_cache(seq_len, rotary_dim, base=10000, device=None,\n                     dtype=torch.float16, style=\"neox\"):\n    assert rotary_dim % 2 == 0\n    device = device or \"cuda\"\n\n    # inv_freq: [rotary_dim/2]\n    i = torch.arange(0, rotary_dim, 2, device=device, dtype=torch.float32)\n    inv_freq = 1.0 / (base ** (i / rotary_dim))\n\n    # t: [seq_len]\n    t = torch.arange(seq_len, device=device, dtype=torch.float32)\n\n    # freqs: [seq_len, rotary_dim/2]\n    freqs = torch.einsum(\"t,f->tf\", t, inv_freq)\n\n    cos = freqs.cos().to(dtype)  # [T, D/2]\n    sin = freqs.sin().to(dtype)  # [T, D/2]\n\n    # expand to [T, D]\n    if style == \"neox\":\n        cos = torch.cat([cos, cos], dim=-1)  # [c.., c..]\n        sin = torch.cat([sin, sin], dim=-1)\n    elif style == \"adjacent\":\n        cos = cos.repeat_interleave(2, dim=-1)  # [c0,c0,c1,c1,...]\n        sin = sin.repeat_interleave(2, dim=-1)\n    else:\n        raise ValueError(f\"unknown style={style}\")\n\n    # broadcast shape for q/k: [B,T,H,rotary_dim]\n    cos = cos[:, None, None, :]  # [T,1,1,D]\n    sin = sin[:, None, None, :]  # [T,1,1,D]\n    return cos, sin\n```\n\n\n### 5.3 应用到 Q/K（只旋转前 rotary_dim）\n\n\n```python\ndef apply_rope(q, k, cos, sin, rotary_dim, style=\"neox\"):\n    # q,k: [B, T, H, D]\n    q1, q2 = q[..., :rotary_dim], q[..., rotary_dim:]\n    k1, k2 = k[..., :rotary_dim], k[..., rotary_dim:]\n\n    rot = rotate_half_neox if style == \"neox\" else rotate_half_adjacent\n\n    q1 = q1 * cos + rot(q1) * sin\n    k1 = k1 * cos + rot(k1) * sin\n\n    q = torch.cat([q1, q2], dim=-1)\n    k = torch.cat([k1, k2], dim=-1)\n    return q, k\n```\n\n\n## 六、设一个最小例子：D = 8（NeoX style）\n\n\n```plain text\nx = [x0, x1, x2, x3, x4, x5, x6, x7]   # 一条 head 的某个 token 的向量\n```\n\n\n`rotate_half(x)` 做什么？\n\n\n```python\nx1 = x[..., :4]   = [x0, x1, x2, x3]\nx2 = x[...,4:]   = [x4, x5, x6, x7]\n\nrotate_half(x) = [-x4, -x5, -x6, -x7, x0, x1, x2, x3]\n```\n\n\n然后 RoPE：\n\n\n```python\nq_rot = x * cos + rotate_half(x) * sin\n```\n\n\n也就是对每个维度 $i$ 都做：\n\n\n```plain text\nq_rot[i] = x[i] * cos[i] + rotate_half(x)[i] * sin[i]\n```\n\n\n用我们上面的 $cos/sin$ 记号 (`cos = [c0..c3,c0..c3]`, `sin=[s0..s3,s0..s3]`)，展开就是：\n\n\n```plain text\nq0 = x0 * c0 + (-x4) * s0 = x0 * c0 - x4 * s0\nq1 = x1 * c1 + (-x5) * s1 = x1 * c1 - x5 * s1\nq2 = x2 * c2 + (-x6) * s2 = x2 * c2 - x6 * s2\nq3 = x3 * c3 + (-x7) * s3 = x3 * c3 - x7 * s3\n\nq4 = x4 * c0 + ( x0) * s0 = x4 * c0 + x0 * s0\nq5 = x5 * c1 + ( x1) * s1 = x5 * c1 + x1 * s1\nq6 = x6 * c2 + ( x2) * s2 = x6 * c2 + x2 * s2\nq7 = x7 * c3 + ( x3) * s3 = x7 * c3 + x3 * s3\n```\n\n\n把结果按 pair 重组一下就非常清晰了。\n\n\n使用频率`(c0,s0)`的是`(x0,x4)`这一对：\n\n\n```plain text\nq0 =  x0 * c0 - x4 * s0\nq4 =  x4 * c0 + x0 * s0\n```\n\n\n这是标准二维旋转：\n\n\n\n$$\n\\begin{bmatrix}q0\\\\q4\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\cos\\theta_0 & -\\sin\\theta_0 \\\\\n\\sin\\theta_0 & \\cos\\theta_0\n\\end{bmatrix}\n\\begin{bmatrix}x0\\\\x4\\end{bmatrix}\n$$\n\n\n\n其他同理：\n\n\n使用`(c1,s1)`的是`(x1,x5)`：\n\n\n```plain text\nq1 =  x1 * c1 - x5 * s1\nq5 =  x5 * c1 + x1 * s1\n```\n\n\n使用`(c2,s2)`的是`(x2,x6)`：\n\n\n```plain text\nq2 =  x2 * c2 - x6 * s2\nq6 =  x6 * c2 + x2 * s2\n```\n\n\n使用`(c3,s3)`的是`(x3,x7)`：\n\n\n```plain text\nq3 =  x3 * c3 - x7 * s3\nq7 =  x7 * c3 + x3 * s3\n```","src/content/blog/notion/rope.md","ce546d297a2be118",{"html":24,"metadata":25},"\u003Chr>\n\u003Ch2 id=\"一rope-到底改了什么\">一、RoPE 到底改了什么？\u003C/h2>\n\u003Cp>Attention 里我们原来用：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>Q\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>K\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">Q, K, V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span> 计算 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmtext>Attn\u003C/mtext>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>Q\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>K\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>V\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\text{Attn}(Q, K, V)\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord text\">\u003Cspan class=\"mord\">Attn\u003C/span>\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/li>\n\u003C/ul>\n\u003Cp>RoPE 做的是把 \u003Cstrong>Q/K 先旋转\u003C/strong>：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsup>\u003Cmi>Q\u003C/mi>\u003Cmo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′\u003C/mo>\u003C/msup>\u003Cmo>=\u003C/mo>\u003Cmtext>RoPE\u003C/mtext>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>Q\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmtext>pos\u003C/mtext>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">Q' = \\text{RoPE}(Q, \\text{pos})\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.9463em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.7519em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">′\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord text\">\u003Cspan class=\"mord\">RoPE\u003C/span>\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord text\">\u003Cspan class=\"mord\">pos\u003C/span>\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/li>\n\u003Cli>\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsup>\u003Cmi>K\u003C/mi>\u003Cmo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′\u003C/mo>\u003C/msup>\u003Cmo>=\u003C/mo>\u003Cmtext>RoPE\u003C/mtext>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>K\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmtext>pos\u003C/mtext>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">K' = \\text{RoPE}(K, \\text{pos})\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.7519em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.7519em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">′\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord text\">\u003Cspan class=\"mord\">RoPE\u003C/span>\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord text\">\u003Cspan class=\"mord\">pos\u003C/span>\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/li>\n\u003Cli>然后用 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsup>\u003Cmi>Q\u003C/mi>\u003Cmo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′\u003C/mo>\u003C/msup>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmsup>\u003Cmi>K\u003C/mi>\u003Cmo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′\u003C/mo>\u003C/msup>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">Q', K', V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.9463em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.7519em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">′\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.7519em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">′\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span> 做 attention\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Ch2 id=\"二数学形式2d-旋转每两维一组\">二、数学形式：2D 旋转（每两维一组）\u003C/h2>\n\u003Cp>对每个 position \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>p\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">p\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">p\u003C/span>\u003C/span>\u003C/span>\u003C/span>，对某一对维度 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>u\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>v\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">(u,v)\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord mathnormal\">u\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span> 做旋转：\u003C/p>\n\u003Cspan class=\"katex-display\">\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\u003Csemantics>\u003Cmrow>\u003Cmrow>\u003Cmo fence=\"true\">[\u003C/mo>\u003Cmtable rowspacing=\"0.16em\" columnalign=\"center\" columnspacing=\"1em\">\u003Cmtr>\u003Cmtd>\u003Cmstyle scriptlevel=\"0\" displaystyle=\"false\">\u003Cmsubsup>\u003Cmi>x\u003C/mi>\u003Cmi>u\u003C/mi>\u003Cmo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′\u003C/mo>\u003C/msubsup>\u003C/mstyle>\u003C/mtd>\u003C/mtr>\u003Cmtr>\u003Cmtd>\u003Cmstyle scriptlevel=\"0\" displaystyle=\"false\">\u003Cmsubsup>\u003Cmi>x\u003C/mi>\u003Cmi>v\u003C/mi>\u003Cmo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′\u003C/mo>\u003C/msubsup>\u003C/mstyle>\u003C/mtd>\u003C/mtr>\u003C/mtable>\u003Cmo fence=\"true\">]\u003C/mo>\u003C/mrow>\u003Cmo>=\u003C/mo>\u003Cmrow>\u003Cmo fence=\"true\">[\u003C/mo>\u003Cmtable rowspacing=\"0.16em\" columnalign=\"center center\" columnspacing=\"1em\">\u003Cmtr>\u003Cmtd>\u003Cmstyle scriptlevel=\"0\" displaystyle=\"false\">\u003Cmrow>\u003Cmi>cos\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003Cmi>θ\u003C/mi>\u003C/mrow>\u003C/mstyle>\u003C/mtd>\u003Cmtd>\u003Cmstyle scriptlevel=\"0\" displaystyle=\"false\">\u003Cmrow>\u003Cmo>−\u003C/mo>\u003Cmi>sin\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003Cmi>θ\u003C/mi>\u003C/mrow>\u003C/mstyle>\u003C/mtd>\u003C/mtr>\u003Cmtr>\u003Cmtd>\u003Cmstyle scriptlevel=\"0\" displaystyle=\"false\">\u003Cmrow>\u003Cmi>sin\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003Cmi>θ\u003C/mi>\u003C/mrow>\u003C/mstyle>\u003C/mtd>\u003Cmtd>\u003Cmstyle scriptlevel=\"0\" displaystyle=\"false\">\u003Cmrow>\u003Cmi>cos\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003Cmi>θ\u003C/mi>\u003C/mrow>\u003C/mstyle>\u003C/mtd>\u003C/mtr>\u003C/mtable>\u003Cmo fence=\"true\">]\u003C/mo>\u003C/mrow>\u003Cmrow>\u003Cmo fence=\"true\">[\u003C/mo>\u003Cmtable rowspacing=\"0.16em\" columnalign=\"center\" columnspacing=\"1em\">\u003Cmtr>\u003Cmtd>\u003Cmstyle scriptlevel=\"0\" displaystyle=\"false\">\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmi>u\u003C/mi>\u003C/msub>\u003C/mstyle>\u003C/mtd>\u003C/mtr>\u003Cmtr>\u003Cmtd>\u003Cmstyle scriptlevel=\"0\" displaystyle=\"false\">\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmi>v\u003C/mi>\u003C/msub>\u003C/mstyle>\u003C/mtd>\u003C/mtr>\u003C/mtable>\u003Cmo fence=\"true\">]\u003C/mo>\u003C/mrow>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\begin{bmatrix}\nx'_u \\\\\nx'_v\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\cos\\theta &#x26; -\\sin\\theta \\\\\n\\sin\\theta &#x26; \\cos\\theta\n\\end{bmatrix}\n\\begin{bmatrix}\nx_u \\\\\nx_v\n\\end{bmatrix}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:2.4em;vertical-align:-0.95em;\">\u003C/span>\u003Cspan class=\"minner\">\u003Cspan class=\"mopen delimcenter\" style=\"top:0em;\">\u003Cspan class=\"delimsizing size3\">[\u003C/span>\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mtable\">\u003Cspan class=\"col-align-c\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.45em;\">\u003Cspan style=\"top:-3.61em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.7519em;\">\u003Cspan style=\"top:-2.453em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">u\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">′\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.247em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-2.41em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.7519em;\">\u003Cspan style=\"top:-2.453em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">v\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">′\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.247em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.95em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose delimcenter\" style=\"top:0em;\">\u003Cspan class=\"delimsizing size3\">]\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:2.4em;vertical-align:-0.95em;\">\u003C/span>\u003Cspan class=\"minner\">\u003Cspan class=\"mopen delimcenter\" style=\"top:0em;\">\u003Cspan class=\"delimsizing size3\">[\u003C/span>\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mtable\">\u003Cspan class=\"col-align-c\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.45em;\">\u003Cspan style=\"top:-3.61em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mop\">cos\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-2.41em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mop\">sin\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.95em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"arraycolsep\" style=\"width:0.5em;\">\u003C/span>\u003Cspan class=\"arraycolsep\" style=\"width:0.5em;\">\u003C/span>\u003Cspan class=\"col-align-c\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.45em;\">\u003Cspan style=\"top:-3.61em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord\">−\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mop\">sin\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-2.41em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mop\">cos\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.95em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose delimcenter\" style=\"top:0em;\">\u003Cspan class=\"delimsizing size3\">]\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"minner\">\u003Cspan class=\"mopen delimcenter\" style=\"top:0em;\">\u003Cspan class=\"delimsizing size3\">[\u003C/span>\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mtable\">\u003Cspan class=\"col-align-c\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.45em;\">\u003Cspan style=\"top:-3.61em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.1514em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">u\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-2.41em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.1514em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">v\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.95em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose delimcenter\" style=\"top:0em;\">\u003Cspan class=\"delimsizing size3\">]\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\n\u003Cp>工程实现常写成统一模板：\u003C/p>\n\u003Cp>\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmtext>⁡\u003C/mtext>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmtext>rot\u003C/mtext>\u003C/msub>\u003Cmo>=\u003C/mo>\u003Cmi>x\u003C/mi>\u003Cmo>⋅\u003C/mo>\u003Cmi>cos\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003Cmo>+\u003C/mo>\u003Cmtext>rotate_half\u003C/mtext>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>x\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003Cmo>⋅\u003C/mo>\u003Cmi>sin\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">⁡x_{\\text{rot}} = x \\cdot \\cos + \\text{rotate\\_half}(x)\\cdot \\sin\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\">\u003C/span>\u003Cspan class=\"mord\">⁡\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.2806em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">rot\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.4445em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003Cspan class=\"mbin\">⋅\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.06em;vertical-align:-0.31em;\">\u003C/span>\u003Cspan class=\"mop\">cos\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">+\u003C/span>\u003Cspan class=\"mord text\">\u003Cspan class=\"mord\">rotate_half\u003C/span>\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003Cspan class=\"mbin\">⋅\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6679em;\">\u003C/span>\u003Cspan class=\"mop\">sin\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/p>\n\u003Cp>关键：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>\u003Ccode>rotate_half\u003C/code>\u003C/strong> \u003Cstrong>定义了“哪两维是一对”\u003C/strong>\u003C/li>\n\u003Cli>\u003Cstrong>\u003Ccode>cos/sin\u003C/code>\u003C/strong> \u003Cstrong>展开方式必须和这个配对一致\u003C/strong>\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Ch2 id=\"三两种-rope-style差别只在怎么配对维度\">三、两种 RoPE style：差别只在“怎么配对维度”\u003C/h2>\n\u003Cp>假设 head_dim \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>D\u003C/mi>\u003Cmo>=\u003C/mo>\u003Cmn>8\u003C/mn>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">D=8\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6444em;\">\u003C/span>\u003Cspan class=\"mord\">8\u003C/span>\u003C/span>\u003C/span>\u003C/span>，向量：\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>x\u003C/mi>\u003Cmo>=\u003C/mo>\u003Cmo stretchy=\"false\">[\u003C/mo>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmn>0\u003C/mn>\u003C/msub>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmn>1\u003C/mn>\u003C/msub>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmn>2\u003C/mn>\u003C/msub>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmn>3\u003C/mn>\u003C/msub>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmn>4\u003C/mn>\u003C/msub>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmn>5\u003C/mn>\u003C/msub>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmn>6\u003C/mn>\u003C/msub>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmn>7\u003C/mn>\u003C/msub>\u003Cmo stretchy=\"false\">]\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">x=[x_0,x_1,x_2,x_3,x_4,x_5,x_6,x_7]\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.4306em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mopen\">[\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3011em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">0\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3011em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">1\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3011em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">2\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3011em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">3\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3011em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">4\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3011em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">5\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3011em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">6\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3011em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">7\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose\">]\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/p>\n\u003Ch3 id=\"31-adjacent-pair相邻配对--even-odd\">3.1 Adjacent-pair（相邻配对 / even-odd）\u003C/h3>\n\u003Cp>配对方式：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmn>0\u003C/mn>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmn>1\u003C/mn>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmn>2\u003C/mn>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmn>3\u003C/mn>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmn>4\u003C/mn>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmn>5\u003C/mn>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmn>6\u003C/mn>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmn>7\u003C/mn>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">(0,1), (2,3), (4,5), (6,7)\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">0\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">1\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">2\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">3\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">4\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">5\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">6\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">7\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/li>\n\u003C/ul>\n\u003Cp>对应的 \u003Ccode>rotate_half\u003C/code>（相邻两两转）会把每对做 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmo>−\u003C/mo>\u003Cmi>y\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>x\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">(−y,x)\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">−\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>：\u003C/p>\n\u003Cul>\n\u003Cli>输出：\u003Ccode>[-x1, x0, -x3, x2, -x5, x4, -x7, x6]\u003C/code>\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"32-neox-style前后半配对--split-half\">3.2 NeoX-style（前后半配对 / split-half）\u003C/h3>\n\u003Cp>配对方式：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmn>0\u003C/mn>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmn>4\u003C/mn>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmn>1\u003C/mn>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmn>5\u003C/mn>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmn>2\u003C/mn>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmn>6\u003C/mn>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmn>3\u003C/mn>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmn>7\u003C/mn>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">(0,4),(1,5),(2,6),(3,7)\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">0\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">4\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">1\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">5\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">2\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">6\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">3\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">7\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/li>\n\u003C/ul>\n\u003Cp>对应的 \u003Ccode>rotate_half\u003C/code>（前后半互换再取负）：\u003C/p>\n\u003Cul>\n\u003Cli>输出：\u003Ccode>[-x4, -x5, -x6, -x7, x0, x1, x2, x3]\u003C/code>\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Ch2 id=\"四最容易踩坑的点cossin-怎么铺满到-d-维\">四、最容易踩坑的点：cos/sin 怎么“铺满到 D 维”\u003C/h2>\n\u003Cp>先算基础的 ⁡\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>cos\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>sin\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\cos,\\sin\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8623em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mop\">cos\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mop\">sin\u003C/span>\u003C/span>\u003C/span>\u003C/span>（每对一个频率），它们天然是长度 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>D\u003C/mi>\u003Cmi mathvariant=\"normal\">/\u003C/mi>\u003Cmn>2\u003C/mn>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">D/2\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D\u003C/span>\u003Cspan class=\"mord\">/2\u003C/span>\u003C/span>\u003C/span>\u003C/span> 的向量：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ccode>cos_base = [c0 c1 c2 c3]\u003C/code>\u003C/li>\n\u003Cli>\u003Ccode>sin_base = [s0 s1 s2 s3]\u003C/code>\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"41-neox-style-的展开拼两份\">4.1 NeoX-style 的展开（\u003Cstrong>拼两份\u003C/strong>）\u003C/h3>\n\u003Cp>因为配对是 \u003Ccode>(i, i+D/2)\u003C/code>，所以要变成：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ccode>cos_full = [c0 c1 c2 c3 c0 c1 c2 c3]\u003C/code>\u003C/li>\n\u003Cli>\u003Ccode>sin_full = [s0 s1 s2 s3 s0 s1 s2 s3]\u003C/code>\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"42-adjacent-pair-的展开每个重复两次\">4.2 Adjacent-pair 的展开（\u003Cstrong>每个重复两次\u003C/strong>）\u003C/h3>\n\u003Cp>因为配对是 \u003Ccode>(2i, 2i+1)\u003C/code>，所以要变成：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ccode>cos_full = [c0 c0 c1 c1 c2 c2 c3 c3]\u003C/code>\u003C/li>\n\u003Cli>\u003Ccode>sin_full = [s0 s0 s1 s1 s2 s2 s3 s3]\u003C/code>\u003C/li>\n\u003C/ul>\n\u003Cp>\u003Cstrong>结论（牢记）\u003C/strong>：\u003C/p>\n\u003Cul>\n\u003Cli>NeoX-style：\u003Ccode>rotate_half = cat([-x2, x1])\u003C/code> 必须配 \u003Ccode>cos/sin = cat([cos, cos])\u003C/code>\u003C/li>\n\u003Cli>Adjacent-pair：\u003Ccode>rotate_half\u003C/code> 用 \u003Ccode>::2 / 1::2\u003C/code> 必须配 \u003Ccode>repeat_interleave(2)\u003C/code>\u003C/li>\n\u003C/ul>\n\u003Cp>不匹配通常会出现：\u003C/p>\n\u003Cblockquote>\n\u003Cp>不报 shape 错，但模型效果/困惑度明显崩掉\u003C/p>\n\u003C/blockquote>\n\u003Chr>\n\u003Ch2 id=\"五参考实现布局b-t-h-d\">五、参考实现（布局：\u003Ccode>[B, T, H, D]\u003C/code>）\u003C/h2>\n\u003Ch3 id=\"51-两种-rotate_half\">5.1 两种 rotate_half\u003C/h3>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">def\u003C/span>\u003Cspan style=\"color:#B392F0\"> rotate_half_adjacent\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(x):  \u003C/span>\u003Cspan style=\"color:#6A737D\"># x: [..., rotary_dim]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">    # pair: (0,1), (2,3), ...\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    x_even \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> x[\u003C/span>\u003Cspan style=\"color:#79B8FF\">...\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, ::\u003C/span>\u003Cspan style=\"color:#79B8FF\">2\u003C/span>\u003Cspan style=\"color:#E1E4E8\">]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    x_odd  \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> x[\u003C/span>\u003Cspan style=\"color:#79B8FF\">...\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">::\u003C/span>\u003Cspan style=\"color:#79B8FF\">2\u003C/span>\u003Cspan style=\"color:#E1E4E8\">]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    out \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.stack((\u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#E1E4E8\">x_odd, x_even), \u003C/span>\u003Cspan style=\"color:#FFAB70\">dim\u003C/span>\u003Cspan style=\"color:#F97583\">=-\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    return\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> out.flatten(\u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#79B8FF\">2\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)  \u003C/span>\u003Cspan style=\"color:#6A737D\"># restore last dim\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">def\u003C/span>\u003Cspan style=\"color:#B392F0\"> rotate_half_neox\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(x):  \u003C/span>\u003Cspan style=\"color:#6A737D\"># x: [..., rotary_dim]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">    # pair: (0, D/2), (1, D/2+1), ...\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    half \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> x.shape[\u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">] \u003C/span>\u003Cspan style=\"color:#F97583\">//\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 2\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    x1, x2 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> x[\u003C/span>\u003Cspan style=\"color:#79B8FF\">...\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, :half], x[\u003C/span>\u003Cspan style=\"color:#79B8FF\">...\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, half:]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    return\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.cat((\u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#E1E4E8\">x2, x1), \u003C/span>\u003Cspan style=\"color:#FFAB70\">dim\u003C/span>\u003Cspan style=\"color:#F97583\">=-\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"52-构建-cossin-cache核心不同-style-的展开\">5.2 构建 cos/sin cache（核心：不同 style 的展开）\u003C/h3>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">def\u003C/span>\u003Cspan style=\"color:#B392F0\"> build_rope_cache\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(seq_len, rotary_dim, base\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">10000\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, device\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">None\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">                     dtype\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">torch.float16, style\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"neox\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    assert\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> rotary_dim \u003C/span>\u003Cspan style=\"color:#F97583\">%\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 2\u003C/span>\u003Cspan style=\"color:#F97583\"> ==\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 0\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    device \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> device \u003C/span>\u003Cspan style=\"color:#F97583\">or\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"cuda\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">    # inv_freq: [rotary_dim/2]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    i \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.arange(\u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, rotary_dim, \u003C/span>\u003Cspan style=\"color:#79B8FF\">2\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">device\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">device, \u003C/span>\u003Cspan style=\"color:#FFAB70\">dtype\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">torch.float32)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    inv_freq \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 1.0\u003C/span>\u003Cspan style=\"color:#F97583\"> /\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (base \u003C/span>\u003Cspan style=\"color:#F97583\">**\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (i \u003C/span>\u003Cspan style=\"color:#F97583\">/\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> rotary_dim))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">    # t: [seq_len]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    t \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.arange(seq_len, \u003C/span>\u003Cspan style=\"color:#FFAB70\">device\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">device, \u003C/span>\u003Cspan style=\"color:#FFAB70\">dtype\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">torch.float32)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">    # freqs: [seq_len, rotary_dim/2]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    freqs \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.einsum(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"t,f->tf\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, t, inv_freq)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    cos \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> freqs.cos().to(dtype)  \u003C/span>\u003Cspan style=\"color:#6A737D\"># [T, D/2]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    sin \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> freqs.sin().to(dtype)  \u003C/span>\u003Cspan style=\"color:#6A737D\"># [T, D/2]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">    # expand to [T, D]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> style \u003C/span>\u003Cspan style=\"color:#F97583\">==\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"neox\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        cos \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.cat([cos, cos], \u003C/span>\u003Cspan style=\"color:#FFAB70\">dim\u003C/span>\u003Cspan style=\"color:#F97583\">=-\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)  \u003C/span>\u003Cspan style=\"color:#6A737D\"># [c.., c..]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        sin \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.cat([sin, sin], \u003C/span>\u003Cspan style=\"color:#FFAB70\">dim\u003C/span>\u003Cspan style=\"color:#F97583\">=-\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    elif\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> style \u003C/span>\u003Cspan style=\"color:#F97583\">==\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"adjacent\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        cos \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> cos.repeat_interleave(\u003C/span>\u003Cspan style=\"color:#79B8FF\">2\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">dim\u003C/span>\u003Cspan style=\"color:#F97583\">=-\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)  \u003C/span>\u003Cspan style=\"color:#6A737D\"># [c0,c0,c1,c1,...]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        sin \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> sin.repeat_interleave(\u003C/span>\u003Cspan style=\"color:#79B8FF\">2\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">dim\u003C/span>\u003Cspan style=\"color:#F97583\">=-\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    else\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        raise\u003C/span>\u003Cspan style=\"color:#79B8FF\"> ValueError\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#F97583\">f\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"unknown style=\u003C/span>\u003Cspan style=\"color:#79B8FF\">{\u003C/span>\u003Cspan style=\"color:#E1E4E8\">style\u003C/span>\u003Cspan style=\"color:#79B8FF\">}\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">    # broadcast shape for q/k: [B,T,H,rotary_dim]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    cos \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> cos[:, \u003C/span>\u003Cspan style=\"color:#79B8FF\">None\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">None\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, :]  \u003C/span>\u003Cspan style=\"color:#6A737D\"># [T,1,1,D]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    sin \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> sin[:, \u003C/span>\u003Cspan style=\"color:#79B8FF\">None\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">None\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, :]  \u003C/span>\u003Cspan style=\"color:#6A737D\"># [T,1,1,D]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    return\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> cos, sin\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"53-应用到-qk只旋转前-rotary_dim\">5.3 应用到 Q/K（只旋转前 rotary_dim）\u003C/h3>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">def\u003C/span>\u003Cspan style=\"color:#B392F0\"> apply_rope\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(q, k, cos, sin, rotary_dim, style\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"neox\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">    # q,k: [B, T, H, D]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    q1, q2 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> q[\u003C/span>\u003Cspan style=\"color:#79B8FF\">...\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, :rotary_dim], q[\u003C/span>\u003Cspan style=\"color:#79B8FF\">...\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, rotary_dim:]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    k1, k2 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> k[\u003C/span>\u003Cspan style=\"color:#79B8FF\">...\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, :rotary_dim], k[\u003C/span>\u003Cspan style=\"color:#79B8FF\">...\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, rotary_dim:]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    rot \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> rotate_half_neox \u003C/span>\u003Cspan style=\"color:#F97583\">if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> style \u003C/span>\u003Cspan style=\"color:#F97583\">==\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"neox\"\u003C/span>\u003Cspan style=\"color:#F97583\"> else\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> rotate_half_adjacent\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    q1 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> q1 \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> cos \u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> rot(q1) \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> sin\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    k1 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> k1 \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> cos \u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> rot(k1) \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> sin\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    q \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.cat([q1, q2], \u003C/span>\u003Cspan style=\"color:#FFAB70\">dim\u003C/span>\u003Cspan style=\"color:#F97583\">=-\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    k \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.cat([k1, k2], \u003C/span>\u003Cspan style=\"color:#FFAB70\">dim\u003C/span>\u003Cspan style=\"color:#F97583\">=-\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    return\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> q, k\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"六设一个最小例子d--8neox-style\">六、设一个最小例子：D = 8（NeoX style）\u003C/h2>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plain\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>x = [x0, x1, x2, x3, x4, x5, x6, x7]   # 一条 head 的某个 token 的向量\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>\u003Ccode>rotate_half(x)\u003C/code> 做什么？\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">x1 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> x[\u003C/span>\u003Cspan style=\"color:#79B8FF\">...\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, :\u003C/span>\u003Cspan style=\"color:#79B8FF\">4\u003C/span>\u003Cspan style=\"color:#E1E4E8\">]   \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [x0, x1, x2, x3]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">x2 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> x[\u003C/span>\u003Cspan style=\"color:#79B8FF\">...\u003C/span>\u003Cspan style=\"color:#E1E4E8\">,\u003C/span>\u003Cspan style=\"color:#79B8FF\">4\u003C/span>\u003Cspan style=\"color:#E1E4E8\">:]   \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [x4, x5, x6, x7]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">rotate_half(x) \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> [\u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#E1E4E8\">x4, \u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#E1E4E8\">x5, \u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#E1E4E8\">x6, \u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#E1E4E8\">x7, x0, x1, x2, x3]\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>然后 RoPE：\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">q_rot \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> x \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> cos \u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> rotate_half(x) \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> sin\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>也就是对每个维度 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>i\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">i\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6595em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span> 都做：\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plain\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>q_rot[i] = x[i] * cos[i] + rotate_half(x)[i] * sin[i]\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>用我们上面的 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>c\u003C/mi>\u003Cmi>o\u003C/mi>\u003Cmi>s\u003C/mi>\u003Cmi mathvariant=\"normal\">/\u003C/mi>\u003Cmi>s\u003C/mi>\u003Cmi>i\u003C/mi>\u003Cmi>n\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">cos/sin\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">cos\u003C/span>\u003Cspan class=\"mord\">/\u003C/span>\u003Cspan class=\"mord mathnormal\">s\u003C/span>\u003Cspan class=\"mord mathnormal\">in\u003C/span>\u003C/span>\u003C/span>\u003C/span> 记号 (\u003Ccode>cos = [c0..c3,c0..c3]\u003C/code>, \u003Ccode>sin=[s0..s3,s0..s3]\u003C/code>)，展开就是：\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plain\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>q0 = x0 * c0 + (-x4) * s0 = x0 * c0 - x4 * s0\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>q1 = x1 * c1 + (-x5) * s1 = x1 * c1 - x5 * s1\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>q2 = x2 * c2 + (-x6) * s2 = x2 * c2 - x6 * s2\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>q3 = x3 * c3 + (-x7) * s3 = x3 * c3 - x7 * s3\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>q4 = x4 * c0 + ( x0) * s0 = x4 * c0 + x0 * s0\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>q5 = x5 * c1 + ( x1) * s1 = x5 * c1 + x1 * s1\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>q6 = x6 * c2 + ( x2) * s2 = x6 * c2 + x2 * s2\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>q7 = x7 * c3 + ( x3) * s3 = x7 * c3 + x3 * s3\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>把结果按 pair 重组一下就非常清晰了。\u003C/p>\n\u003Cp>使用频率\u003Ccode>(c0,s0)\u003C/code>的是\u003Ccode>(x0,x4)\u003C/code>这一对：\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plain\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>q0 =  x0 * c0 - x4 * s0\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>q4 =  x4 * c0 + x0 * s0\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>这是标准二维旋转：\u003C/p>\n\u003Cspan class=\"katex-display\">\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\u003Csemantics>\u003Cmrow>\u003Cmrow>\u003Cmo fence=\"true\">[\u003C/mo>\u003Cmtable rowspacing=\"0.16em\" columnalign=\"center\" columnspacing=\"1em\">\u003Cmtr>\u003Cmtd>\u003Cmstyle scriptlevel=\"0\" displaystyle=\"false\">\u003Cmrow>\u003Cmi>q\u003C/mi>\u003Cmn>0\u003C/mn>\u003C/mrow>\u003C/mstyle>\u003C/mtd>\u003C/mtr>\u003Cmtr>\u003Cmtd>\u003Cmstyle scriptlevel=\"0\" displaystyle=\"false\">\u003Cmrow>\u003Cmi>q\u003C/mi>\u003Cmn>4\u003C/mn>\u003C/mrow>\u003C/mstyle>\u003C/mtd>\u003C/mtr>\u003C/mtable>\u003Cmo fence=\"true\">]\u003C/mo>\u003C/mrow>\u003Cmo>=\u003C/mo>\u003Cmrow>\u003Cmo fence=\"true\">[\u003C/mo>\u003Cmtable rowspacing=\"0.16em\" columnalign=\"center center\" columnspacing=\"1em\">\u003Cmtr>\u003Cmtd>\u003Cmstyle scriptlevel=\"0\" displaystyle=\"false\">\u003Cmrow>\u003Cmi>cos\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003Cmsub>\u003Cmi>θ\u003C/mi>\u003Cmn>0\u003C/mn>\u003C/msub>\u003C/mrow>\u003C/mstyle>\u003C/mtd>\u003Cmtd>\u003Cmstyle scriptlevel=\"0\" displaystyle=\"false\">\u003Cmrow>\u003Cmo>−\u003C/mo>\u003Cmi>sin\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003Cmsub>\u003Cmi>θ\u003C/mi>\u003Cmn>0\u003C/mn>\u003C/msub>\u003C/mrow>\u003C/mstyle>\u003C/mtd>\u003C/mtr>\u003Cmtr>\u003Cmtd>\u003Cmstyle scriptlevel=\"0\" displaystyle=\"false\">\u003Cmrow>\u003Cmi>sin\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003Cmsub>\u003Cmi>θ\u003C/mi>\u003Cmn>0\u003C/mn>\u003C/msub>\u003C/mrow>\u003C/mstyle>\u003C/mtd>\u003Cmtd>\u003Cmstyle scriptlevel=\"0\" displaystyle=\"false\">\u003Cmrow>\u003Cmi>cos\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003Cmsub>\u003Cmi>θ\u003C/mi>\u003Cmn>0\u003C/mn>\u003C/msub>\u003C/mrow>\u003C/mstyle>\u003C/mtd>\u003C/mtr>\u003C/mtable>\u003Cmo fence=\"true\">]\u003C/mo>\u003C/mrow>\u003Cmrow>\u003Cmo fence=\"true\">[\u003C/mo>\u003Cmtable rowspacing=\"0.16em\" columnalign=\"center\" columnspacing=\"1em\">\u003Cmtr>\u003Cmtd>\u003Cmstyle scriptlevel=\"0\" displaystyle=\"false\">\u003Cmrow>\u003Cmi>x\u003C/mi>\u003Cmn>0\u003C/mn>\u003C/mrow>\u003C/mstyle>\u003C/mtd>\u003C/mtr>\u003Cmtr>\u003Cmtd>\u003Cmstyle scriptlevel=\"0\" displaystyle=\"false\">\u003Cmrow>\u003Cmi>x\u003C/mi>\u003Cmn>4\u003C/mn>\u003C/mrow>\u003C/mstyle>\u003C/mtd>\u003C/mtr>\u003C/mtable>\u003Cmo fence=\"true\">]\u003C/mo>\u003C/mrow>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\begin{bmatrix}q0\\\\q4\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\cos\\theta_0 &#x26; -\\sin\\theta_0 \\\\\n\\sin\\theta_0 &#x26; \\cos\\theta_0\n\\end{bmatrix}\n\\begin{bmatrix}x0\\\\x4\\end{bmatrix}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:2.4em;vertical-align:-0.95em;\">\u003C/span>\u003Cspan class=\"minner\">\u003Cspan class=\"mopen delimcenter\" style=\"top:0em;\">\u003Cspan class=\"delimsizing size3\">[\u003C/span>\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mtable\">\u003Cspan class=\"col-align-c\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.45em;\">\u003Cspan style=\"top:-3.61em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q\u003C/span>\u003Cspan class=\"mord\">0\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-2.41em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">q\u003C/span>\u003Cspan class=\"mord\">4\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.95em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose delimcenter\" style=\"top:0em;\">\u003Cspan class=\"delimsizing size3\">]\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:2.4em;vertical-align:-0.95em;\">\u003C/span>\u003Cspan class=\"minner\">\u003Cspan class=\"mopen delimcenter\" style=\"top:0em;\">\u003Cspan class=\"delimsizing size3\">[\u003C/span>\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mtable\">\u003Cspan class=\"col-align-c\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.45em;\">\u003Cspan style=\"top:-3.61em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mop\">cos\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3011em;\">\u003Cspan style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">0\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-2.41em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mop\">sin\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3011em;\">\u003Cspan style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">0\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.95em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"arraycolsep\" style=\"width:0.5em;\">\u003C/span>\u003Cspan class=\"arraycolsep\" style=\"width:0.5em;\">\u003C/span>\u003Cspan class=\"col-align-c\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.45em;\">\u003Cspan style=\"top:-3.61em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord\">−\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mop\">sin\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3011em;\">\u003Cspan style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">0\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-2.41em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mop\">cos\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3011em;\">\u003Cspan style=\"top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">0\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.95em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose delimcenter\" style=\"top:0em;\">\u003Cspan class=\"delimsizing size3\">]\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"minner\">\u003Cspan class=\"mopen delimcenter\" style=\"top:0em;\">\u003Cspan class=\"delimsizing size3\">[\u003C/span>\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mtable\">\u003Cspan class=\"col-align-c\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.45em;\">\u003Cspan style=\"top:-3.61em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"mord\">0\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-2.41em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"mord\">4\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.95em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose delimcenter\" style=\"top:0em;\">\u003Cspan class=\"delimsizing size3\">]\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\n\u003Cp>其他同理：\u003C/p>\n\u003Cp>使用\u003Ccode>(c1,s1)\u003C/code>的是\u003Ccode>(x1,x5)\u003C/code>：\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plain\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>q1 =  x1 * c1 - x5 * s1\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>q5 =  x5 * c1 + x1 * s1\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>使用\u003Ccode>(c2,s2)\u003C/code>的是\u003Ccode>(x2,x6)\u003C/code>：\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plain\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>q2 =  x2 * c2 - x6 * s2\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>q6 =  x6 * c2 + x2 * s2\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>使用\u003Ccode>(c3,s3)\u003C/code>的是\u003Ccode>(x3,x7)\u003C/code>：\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"plain\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>q3 =  x3 * c3 - x7 * s3\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>q7 =  x7 * c3 + x3 * s3\u003C/span>\u003C/span>\u003C/code>\u003C/pre>",{"headings":26,"localImagePaths":68,"remoteImagePaths":69,"frontmatter":70,"imagePaths":74},[27,31,34,37,41,44,47,50,53,56,59,62,65],{"depth":28,"slug":29,"text":30},2,"一rope-到底改了什么","一、RoPE 到底改了什么？",{"depth":28,"slug":32,"text":33},"二数学形式2d-旋转每两维一组","二、数学形式：2D 旋转（每两维一组）",{"depth":28,"slug":35,"text":36},"三两种-rope-style差别只在怎么配对维度","三、两种 RoPE style：差别只在“怎么配对维度”",{"depth":38,"slug":39,"text":40},3,"31-adjacent-pair相邻配对--even-odd","3.1 Adjacent-pair（相邻配对 / even-odd）",{"depth":38,"slug":42,"text":43},"32-neox-style前后半配对--split-half","3.2 NeoX-style（前后半配对 / split-half）",{"depth":28,"slug":45,"text":46},"四最容易踩坑的点cossin-怎么铺满到-d-维","四、最容易踩坑的点：cos/sin 怎么“铺满到 D 维”",{"depth":38,"slug":48,"text":49},"41-neox-style-的展开拼两份","4.1 NeoX-style 的展开（拼两份）",{"depth":38,"slug":51,"text":52},"42-adjacent-pair-的展开每个重复两次","4.2 Adjacent-pair 的展开（每个重复两次）",{"depth":28,"slug":54,"text":55},"五参考实现布局b-t-h-d","五、参考实现（布局：[B, T, H, D]）",{"depth":38,"slug":57,"text":58},"51-两种-rotate_half","5.1 两种 rotate_half",{"depth":38,"slug":60,"text":61},"52-构建-cossin-cache核心不同-style-的展开","5.2 构建 cos/sin cache（核心：不同 style 的展开）",{"depth":38,"slug":63,"text":64},"53-应用到-qk只旋转前-rotary_dim","5.3 应用到 Q/K（只旋转前 rotary_dim）",{"depth":28,"slug":66,"text":67},"六设一个最小例子d--8neox-style","六、设一个最小例子：D = 8（NeoX style）",[],[],{"title":14,"slug":11,"date":71,"tags":72,"status":18,"excerpt":17,"cover":17,"notionId":19,"lastEditedTime":73},"2025-12-25",[],"2025-12-25T16:35:00.000Z",[],"notion/rope.md","flashattention",{"id":76,"data":78,"body":83,"filePath":84,"digest":85,"rendered":86,"legacyId":151},{"title":79,"date":80,"tags":81,"excerpt":17,"cover":17,"status":18,"notionId":82},"FlashAttention 原理与实现",["Date","2025-12-25T00:00:00.000Z"],[],"1fb22dca-4210-80cd-a96e-e32787cfd674","---\n\n\n## 学习链接\n\n- [图解大模型计算加速系列：FlashAttention V1，从硬件到计算逻辑 - 知乎](https://zhuanlan.zhihu.com/p/669926191)\n- [图解大模型计算加速系列：FlashAttention V2，从原理到并行计算 - 知乎](https://zhuanlan.zhihu.com/p/691067658)\n- [\\[Attention优化\\]\\[2w字\\]📚原理篇: 从Online-Softmax到FlashAttention V1/V2/V3 - 知乎](https://zhuanlan.zhihu.com/p/668888063?share_code=15umGexBTQvYV&utm_psn=1985738448250896443)\n- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/pdf/2307.08691)\n- [flash attention v1-v3系列论文解读\\[all\\] - 知乎](https://zhuanlan.zhihu.com/p/1951775373198091592?share_code=1j8zn2LiuCsMG&utm_psn=1984332043031713739)\n- [CUDA-MODE课程笔记 第12课，Flash Attention](https://mp.weixin.qq.com/s/IBeBHO5WlS5BfyL0nZaDHg)\n- [Understanding Flash Attention: Writing Triton Kernel Code](https://alexdremov.me/understanding-flash-attention-writing-the-algorithm-from-scratch-in-triton/)\n- [ELI5: FlashAttention. Step by step explanation of how one of… | by Aleksa Gordić | Medium](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)\n- [CUDA 学习：FlashAttention-2 (Part 1) - 知乎](https://zhuanlan.zhihu.com/p/1973861893090321085?share_code=16ebSp1cEFl2g&utm_psn=1978981878536086595)\n- [Flash Attention深度解析 - 知乎](https://zhuanlan.zhihu.com/p/1975300779821773120?share_code=rxemUGRPa6M8&utm_psn=1975339998430638923)\n- [Flash Attention 全解析（上）：从 V1、V2 到 Flash Decoding 的演进与思想 - 知乎](https://zhuanlan.zhihu.com/p/1953761827025584899?share_code=EaB9P4s0GMg6&utm_psn=1953996076680999196)\n- [Flash Attention 全解析(下)：从硬件、算法到指令，榨干 H100 的异步Flash Attention V3 - 知乎](https://zhuanlan.zhihu.com/p/1954119871420933465)\n- [\\[Decoding优化\\]🔥原理&图解FlashDecoding/FlashDecoding++ - 知乎](https://zhuanlan.zhihu.com/p/696075602)\n- [FlashAttention V3 论文原理分析 - 知乎](https://zhuanlan.zhihu.com/p/18986650584)\n- [flash attention的CUDA实现探讨-V3 - 知乎](https://zhuanlan.zhihu.com/p/697515825?share_code=10fP0tgWgdStv&utm_psn=1938149751125161161)\n\n---\n\n\n## 一、引言：Attention 机制与内存瓶颈背景\n\n\n自 Transformer 提出以来，“自注意力（Self-Attention）”已经成为大型模型的核心组件。其基本公式为：\n\n\n$O = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d}}\\right) V$\n\n\n其中 $Q, K, V$ 分别是查询、键、值矩阵，$d$ 是每个注意力头的维度。这个计算需要先计算 $QK^T$ 得到 $N \\times N$ 的注意力得分矩阵，再对每一行执行 Softmax 归一化，最后与 $V$ 相乘得到输出 $O$。如图 1 所示，标准实现中，这三个步骤通常拆分为独立的矩阵运算，会产生大量中间结果。\n\n\n![图 1](/images/notion/1fb22dca-4210-80cd-a96e-e32787cfd674/2d022dca-4210-80ec-a195-c3adbd923096.png)\n\n\n**内存与计算挑战：** 注意力的时间和空间复杂度均为 $O(N^2)$，当序列长度 $N$ 增大时，内存占用和数据传输量会呈二次方增长。例如，长度翻倍会导致注意力矩阵元素数量增加四倍。这导致 GPU 上**内存访问**成为瓶颈——对巨大的 $QK^T$ 矩阵和 Softmax 中间结果的反复读写使计算受限于内存带宽。事实上，在大模型推理中，尽管 GPU 算力很强，**显存的读写速度**往往限制了注意力层的性能。\n\n\n此外，Softmax 计算本身也存在**数值稳定**问题：直接对大数取指数可能溢出，需要减去最大值来稳定计算，这通常需要多次遍历数据（例如“三遍法”分别计算最大值、指数和归一化）。总的来说，**注意力的内存开销和 Softmax 处理**成为 Transformer 扩展长序列时亟待解决的瓶颈。\n\n\nFlashAttention 正是在这种背景下诞生：通过**重排计算顺序和融合算子**，以 **IO（内存访问）优化**为核心，使注意力计算的内存占用从二次降到线性，并显著提升实际运行速度。下文将详细介绍 FlashAttention 的各个版本（V1、V2、V3）的算法原理与实现优化。\n\n\n---\n\n\n## 二、FlashAttention V1：块式算法与 Online Softmax 优化\n\n\nFlashAttention V1（最初发表于 2022 年）是 Tri Dao 等人提出的**精确且内存高效**的注意力计算算法。它的核心思想是在**不引入近似**的前提下，通过**块式（block-wise）计算**和**在线 Softmax** 技巧，将注意力的中间结果限制在高速缓存中，从而减少显存读写。\n\n\n![](/images/notion/1fb22dca-4210-80cd-a96e-e32787cfd674/2d222dca-4210-808b-8f4d-e92e18a8d8b6.png)\n\n\n![图 2：FlashAttention V1 的块式注意力计算原理示意图。将长序列的 K, V 拆分成多个块分批加载至片上高速内存（SRAM）计算，与每个块对应的 Q 批次进行乘积并软最大归一化，再累积输出结果。蓝色框表示存储在 GPU 显存（HBM）的大矩阵未被 materialize，橙色虚线框表示在片上 SRAM 中计算的部分。最终通过重新缩放确保整体 Softmax 正确归一化。](/images/notion/1fb22dca-4210-80cd-a96e-e32787cfd674/2cf22dca-4210-801c-9beb-dddea5d7e160.png)\n\n\n### 2.1 块式计算与内存优化\n\n\n**块式（tiling）计算：** FlashAttention 将 $Q, K, V$ 划分为适合片上缓存的小块，逐块完成注意力计算。具体而言，算法每次从显存（HBM）加载一块 $K$ 和对应的 $V$（例如块大小 $B \\times d$）到共享内存，然后对这块数据与所有查询 $Q$ 分块进行计算。与传统方法一次性计算整个 $QK^T$ 不同，FlashAttention **从不在显存中构造完整的** $N \\times N$ **注意力矩阵**。取而代之，它在片上 SRAM 中**分块计算并及时消耗**这些局部结果。例如，假设将 $K,V$ 各分成两块：先计算第一块 $K^{(1)}, V^{(1)}$ 对输出的贡献，再计算第二块 $K^{(2)}, V^{(2)}$，最后将两部分结果正确叠加。由于**不写入/读取大中间矩阵**，这种分块策略使所需内存读写量从 $O(N^2)$ 降至 $O(N \\cdot d)$，大幅降低了GPU全局内存访问。实践中，这种 IO 优化带来了约 **2～4倍** 的时间加速。更重要的是，**内存占用从二次降为线性**，使得长序列（如数万长度）在有限显存中成为可能。\n\n\n**融合与单Kernel实现：** FlashAttention 将注意力的多步计算融合进**单个 CUDA 核函数**。传统实现需要依次启动独立 kernel 计算 $QK^T$、Softmax、$AV$，期间中间结果多次写回显存。FlashAttention V1 则通过一个内核完成**点积、Softmax 归一化、再乘** $V$ **的完整流程，并处理必要的 mask 操作。这消除了内核间切换和同步开销，也避免了不同 kernel 之间反复读写显存。这一“kernel 融合”手段结合块式计算，使得注意力的计算完全在片上缓存中进行，大幅减少了慢速 HBM 的访问。总之，FlashAttention V1 充分利用 GPU 内存层次结构（寄存器、共享内存 vs. 全局显存），通过以算代存**来提升性能——牺牲一些额外的算术操作来避免昂贵的内存 I/O。\n\n\n### 2.2 Online Softmax：把“全行归一化”拆成“逐块可合并”的稳定统计量\n\n\n块式 attention 的最大障碍在 Softmax：对每个 query 行 $i$，Softmax 需要整行 logits $S_i = Q_i K^\\top$ 的 **全局最大值**与**全局指数和**：\n\n\n$\\mathrm{softmax}(S_i)_t = \\frac{e^{S_{it}}}{\\sum_{u} e^{S_{iu}}}$\n\n\n如果我们把 $K,V$ 沿序列维分成多个块（block），那么每次只看到 logits 的一个子向量 $S_i^{(j)}$。想要不 materialize 全量 $S_i$，就必须支持“**看到一块就更新一次 Softmax 的归一化信息**”。\n\n\nFlashAttention 的做法是：为每个 query 行维护两类 **可增量合并** 的稳定统计量：\n\n- $m_i$：到目前为止见过的 logits 的**运行最大值**（running max）\n- $\\ell_i$：到目前为止见过的 logits 的**稳定指数和**（running exp-sum），也可以理解为 ⁡$\\exp$ 空间的分母，但总是以 $(\\cdot - m_i)$ 的形式存储\n\n这对应经典 log-sum-exp（LSE）稳定形式：\n\n\n\n$$\nm = \\max_t x_t,\\quad\n\\ell = \\sum_t e^{x_t - m},\\quad\n\\log \\sum_t e^{x_t} = m + \\log \\ell\n$$\n\n\n\n假设我们已经处理完前 $j-1$ 个块，得到 $(m_{\\text{old}}, \\ell_{\\text{old}})$。现在处理第 $j$ 个块，先算它自己的局部统计量：\n\n\n\n$$\nm_{\\text{blk}} = \\max(S^{(j)}),\\quad\n\\ell_{\\text{blk}} = \\sum e^{S^{(j)} - m_{\\text{blk}}}\n$$\n\n\n\n合并时，新全局最大值就是：\n\n\n$m_{\\text{new}}=\\max(m_{\\text{old}}, m_{\\text{blk}})$\n\n\n关键在于：$\\ell$ 的基准从旧的 $m_{\\text{old}}$ 切换到了新的 $m_{\\text{new}}$，所以要对旧的累积指数和做一次“重标定（rescale）”：\n\n\n\n$$\n\\ell_{\\text{new}}\n=\n\\ell_{\\text{old}} \\cdot e^{m_{\\text{old}}-m_{\\text{new}}}\n+\n\\ell_{\\text{blk}} \\cdot e^{m_{\\text{blk}}-m_{\\text{new}}}\n$$\n\n\n\n这一条就是 Online Softmax 的核心：**你不需要保存任何历史 logits，只要保存** $(m,\\ell)$**，就能把新的块稳定地合并进去。**\n\n\n### 2.3 累积输出：把 $\\sum e^{S}V$ 也做成“可合并”的形式\n\n\nSoftmax 的最终输出是：$O = \\frac{\\sum_t e^{S_t} V_t}{\\sum_t e^{S_t}}$\n\n\n如果我们只更新分母 $\\ell$ 还不够，还得更新分子（加权和值）。FlashAttention 做法是维护一个未归一化的累计分子（但同样用稳定基准）：\n\n- $Acc$：到目前为止的**稳定加权和**（running weighted sum）\n\n对第 $j$ 块，先算局部未归一化权重：$P^{(j)} = e^{S^{(j)} - m_{\\text{new}}}$，然后做两件事：\n\n1. 把旧的累计分子从基准 $m_{\\text{old}}$ 切到 $m_{\\text{new}}$（同样 rescale）\n2. 加上当前块的贡献 $P^{(j)}V^{(j)}$\n\n写成公式就是：\n$$\nAcc_{\\text{new}}\n=\nAcc_{\\text{old}} \\cdot e^{m_{\\text{old}}-m_{\\text{new}}}\n+\n\\left(P^{(j)} V^{(j)}\\right)\n$$\n，最终输出在所有块结束后一次性归一化：$O = \\frac{Acc}{\\ell}$。\n\n\n```python\ndef online_softmax_blocked(Q_block, K, V, Bc):\n\t\t# Q_block: [Br, d], K: [T, d], V: [T, d]\n    Br, d = Q_block.shape\n    m = np.full((Br,), -np.inf, dtype=np.float32) # running max\n    l = np.zeros((Br,), dtype=np.float32) # running exp-sum\n    acc = np.zeros((Br, d), dtype=np.float32) # running weighted sum\n\n\t\tfor start in range(0, K.shape[0], Bc):\n\t\t    K_blk = K[start:start+Bc] # [Bc, d]\n        V_blk = V[start:start+Bc] # [Bc, d]\n\n        S = (Q_block @ K_blk.T) * scale # [Br, Bc]\n        S = apply_mask_if_needed(S) # causal/pad -> -inf\n\n        m_blk = S.max(axis=1) # [Br]\n        m_new = np.maximum(m, m_blk) # [Br]\n\n\t\t\t\t# rescale old stats to the new max\n        exp_m = np.exp(m - m_new) # [Br]\n        l *= exp_m\n        acc *= exp_m[:,None]\n\n\t\t\t\t# add contribution of this block under the new max\n        P = np.exp(S - m_new[:,None]) # [Br, Bc]\n        l += P.sum(axis=1) # [Br]\n        acc += P @ V_blk # [Br, d]\n\n        m = m_new\n\n    O = acc / l[:,None]\n    LSE = m + np.log(l)\nreturn O, LSE\n```\n\n\n---\n\n\n## 三、FlashAttention V2：并行化与 Kernel 设计改进\n\n\n随着模型上下文长度不断攀升（32K 甚至 128K），FlashAttention V1 尽管高效，但仍有进一步优化空间。FlashAttention V2（2023年提出）在保持V1 精确和内存高效特性的基础上，针对 GPU 实现引入了**更优的并行策略和工作划分**，从而实现**近2倍**的加速。其主要改进包括：减少非矩阵计算开销、更好地利用 GPU 并行资源，以及改进线程内的工作分配。\n\n\n### **3.1 减少非矩阵运算开销**\n\n\n现代 GPU 上的 Tensor Core 对矩阵乘法有极高吞吐，而标量操作（除法、指数、比较等）则慢得多。FlashAttention V2 重新梳理了 V1 中的 Softmax 计算，将一些频繁执行的重缩放（rescale）、边界检查和掩码操作等尽量移出主循环或合并，减少了这些**非矩阵类 FLOP** 的次数。例如，在 Online Softmax 过程中，只在必要时做归一化调整，避免对每块重复做比例缩放。又如将软掩码的检查融入计算流程，尽量利用 GPU 的向量化指令替代显式判断。这些改动虽然不改变算法结果，但节省了很多 GPU 上“慢指令”的开销。据统计，Ampere 架构下一个 FP32 除法/指数等特殊函数的吞吐仅是 Tensor Core 矩阵乘法的 1/16 甚至更低。因此，通过减少这些操作，V2 能让 GPU 更多时间用于高速的矩阵乘法，从而提高整体 FLOPs 利用率。\n\n\n### **3.2 跨序列长度的并行**\n\n\nFlashAttention V1 的并行化维度主要是**批次（batch）和多头（head）**。它为每个注意力头启动一个线程块（block），总共启动`batch_size * num_heads`个线程块并行计算。在典型训练场景（大batch、多头）下，这样可以占满 GPU 的大部分 SM。然而在**长序列-小 batch** 的推理或训练下，V1 常出现 GPU 资源利用不充分的问题：比如 batch size 很小且序列很长时，线程块数不足以占用全部 SM。V2 针对这一情况，增加了沿**序列长度方向的并行**。具体做法是在前向计算中，将注意力矩阵按“查询序列”的行拆分，由多个线程块分别处理不同的行块。这样，即使 batch 很小，每个 head 的长序列也能拆分成多个部分并行计算，从而大幅提高 SM 占用率。如**图 3** 所示，在前向过程中，不同线程块（Worker）各负责注意力矩阵的一部分行；在反向过程中，则各负责一部分列，以避免竞争更新梯度时的冲突。这种沿序列拆分的调度使 FlashAttention 在极长序列、小批量情况下仍能接近满 GPU 并行度，从而**支持更长上下文**并提升此情形下的速度。\n\n\n![图 3：FlashAttention-2 中跨线程块的并行调度示意。左图为前向传播：每个“Worker”（线程块）负责注意力矩阵的一块行片段，例如 Worker1 处理红色行块，Worker2 处理粉色行块等。右图为反向传播：每个线程块负责一块列片段，例如 Worker1 处理红色列块，Worker2 处理粉色列块等。这种沿序列长度的划分提高了长序列小批量情况下 GPU 的利用率。](/images/notion/1fb22dca-4210-80cd-a96e-e32787cfd674/2d022dca-4210-8038-a862-fb77cf289cdf.png)\n\n\n### **3.3 改进线程块内的 Warp 分工**\n\n\n在 GPU 上，一个线程块包含若干个 Warp（每个 Warp 32 个线程）。在 FlashAttention V1 中，采用的是“**切分 K（split-K）**”的方案：即每个 Warp 各自处理一部分 $K$ 和对应的 $V$，计算部分的 $QK^T$，然后需要将各 Warp 的结果写入共享内存并同步，加和得到完整输出。这种方案需要 Warps 间频繁同步和共享内存通信，造成一定开销。FlashAttention V2 改为“**切分 Q（split-Q）**”方案：让每个 Warp 处理**不同的查询** $Q$ **子块**，而使所有 Warp 都能访问完整的 $K$ 和 $V$ 块。这样，每个 Warp 可以独立完成自己那部分 $QK^T$ 乘积并直接乘以共享的 $V$ 得到对应输出片段，无需与其他 Warp 交换中间结果。**图 4** 展示了两种 Warp 分工方式的对比：FlashAttention-1（左）中不同颜色 Warp 各处理一部分 $K,V$，需要写共享内存（圆圈部分）再汇总；FlashAttention-2（右）则每个 Warp 处理不同的 $Q$ 行，避免了 Warp 间通信。这种改进显著减少了片内共享内存的读写和同步屏障，使单个线程块内部执行更高效。\n\n\n![图 4：Warp 级工作划分对比。（左）FlashAttention-1 中采用“切分 K”方案，不同 Warp 处理不同列块（K 块），需要在共享内存同步累加。（右）FlashAttention-2 改为“切分 Q”方案，不同 Warp 处理不同行块（Q 块），各 Warp 直接算出自己负责的输出部分，无需 Warp 间通信。新方案减少了共享内存读写和同步，提升了效率。](/images/notion/1fb22dca-4210-80cd-a96e-e32787cfd674/2d022dca-4210-8014-8fb1-c14621251b02.png)\n\n\n### **3.4 支持更大 Head 维度与多查询注意力**\n\n\nFlashAttention V1 限制每个注意力头维度 $d \\le 128$，而一些模型使用到 256 维头。V2 通过内存管理优化支持了 **head 尺寸扩展到 256**。此外，V2 新增对多查询注意力（MQA）**和**分组查询注意力（GQA）的支持。这些变体在推理时共享 Key/Value 以减少缓存大小，FlashAttention-2 通过调整索引机制实现了对它们的适配，而无需为每个头单独存储完整的 $K,V$。\n\n\n**实现与性能：** FlashAttention-2 基于 NVIDIA 的 CUTLASS 3.x 和 CuTe 库完全重写，实现了高度优化的 CUDA 内核。相较 V1 的定制 CUDA 实现，新版本在寄存器利用和指令级优化上更进一步，并减少了开发复杂度。值得一提的是，社区也有使用 Triton 实现 FlashAttention 的尝试，但 FlashAttention-2 的官方实现性能更胜一筹，可达原 Triton 版本 1.3～2.5 倍速度。综合上述改进，FlashAttention-2 在 A100 GPU 上的前向+后向总速度达到原版的 2 倍左右，FLOPs 利用率提升到理论峰值的 50～73%（约 230 TFLOPs/s，FP16）。在端到端训练 GPT 等模型时，单卡可稳定运行约 225 TFLOPs/s，相当于 72% 的模型 FLOPs 利用率。同时显存占用仍与序列长度线性关系，保持了大幅节省内存的优势。总结来说，FlashAttention-2 **通过更佳的并行和更精细的 kernel 设计**进一步压榨了 GPU 性能，使得即使在极长序列或硬件升级的场景下，注意力层依然高效。\n\n\n---\n\n\n## 四、FlashAttention V3：异步流水线与推理优化\n\n\n2024 年，FlashAttention 家族又迎来两项重要进展：**FlashAttention-3** 针对最新 H100 等硬件引入**异步流水线**和**低精度优化**，将训练推理速度再次提升 1.5～2 倍。\n\n\nNVIDIA Hopper 架构（如 H100 GPU）引入了若干新特性：Warp 级并行 GEMM 指令（WGMMA）、Tensor Memory Accelerator（TMA）以及更高效的 FP8 矩阵运算等。FlashAttention-3 利用这些硬件能力，实现了**计算与数据传输的重叠并行**以及**更低精度下的高准确率**，从而在 H100 上将注意力计算的 FLOPs 利用率提高到约 75%（FP16/BF16）甚至 85%（FP8 模式）。\n\n\n### **4.1 异步计算与流水线重叠**\n\n\n传统注意力计算流程中，矩阵乘法（GEMM）和 Softmax 是串行的：必须先算完所有 $QK^T$ 得到注意力得分，再计算 Softmax，再乘 $V$。然而在 H100上，**GEMM 和 Softmax 两类操作可以并行重叠**执行。原因在于它们使用 GPU 上不同的计算单元（Tensor Core vs. 标量单元），例如 H100 的 FP16 Tensor Core 峰值 989 TFLOPs，而执行指数的标量单元仅 ~3.9 TFLOPs，相差达 256 倍。Softmax 尽管 FLOPs 占比不高，但因为速度慢，如果串行执行会占用总时间约一半。理想状况下，我们希望**在 Tensor Core 做矩阵乘法的同时，利用其他单元并行计算Softmax**。\n\n\n![图 5：FlashAttention-3 中跨 Warp 组的 Ping-Pong 异步调度。Warp 组 1（上方）和 Warp 组 2（下方）交替执行矩阵乘 GEMM 和 Softmax 操作：当一组等待矩阵乘结果时，另一组利用空闲执行 Softmax。虚线分隔不同迭代，彩色块表示 GEMM 或 Softmax 所占用的时间段。这样实现两类操作重叠并行，提高了硬件利用率。](/images/notion/1fb22dca-4210-80cd-a96e-e32787cfd674/2d022dca-4210-8058-95fe-d57ccd03d3b6.png)\n\n\nFlashAttention-3 通过 Warp 级专业化（warp specialization）实现这一点：将同一线程块内的 Warp 分成两个角色，一部分 Warp 专职执行 GEMM（通过新 WGMMA 指令提高吞吐），另一部分 Warp 专职执行 Softmax 和归一化。然后采用“**Ping-Pong 调度**”在 Warp 组间交替执行：例如两个 Warp 组，一组先进行当前块的 GEMM 计算，另一组利用这段时间对上一个块执行 Softmax；随后两组交换角色，如此往复。**图 5** 展示了有两个 Warp 组时的流水线时间表：相同颜色代表同一次迭代，Warp 组 1 先执行 GEMM0 然后 Softmax，而 Warp 组 2 稍后开始 GEMM0，当 Warp 组 1 切换去执行 GEMM1 时，Warp组 2 正好执行 Softmax，以此交错重叠。这种手动的 barrier 调度让两个 Warp 组总有一个在算 Softmax 而另一个在算 GEMM，从而把 Softmax 隐藏在别的计算“阴影”下，提升流水线并行度。实测在 H100 上，FP16 前向算力通过这种 Warp 组 Ping-Pong 调度，速度从 570 TFLOPs 提高到 620 TFLOPs 左右。\n\n\n![图 6](/images/notion/1fb22dca-4210-80cd-a96e-e32787cfd674/2d022dca-4210-808c-928e-d45bfea290a2.png)\n\n\n除了 Warp 组之间的并行，如图 6 所示，FlashAttention-3 还进一步在**单个 Warp 组内实现流水线**：将一次 Attention 计算拆成两个阶段，先执行部分 GEMM 累积，再插入 Softmax 计算，然后继续下一个 GEMM。通过在 Warp 内交叉执行，小部分 Softmax 计算可以与本组后续的 GEMM 重叠，从而进一步榨取并行度。这种方案增加了一些寄存器开销（需要同时保存 GEMM 累积和 Softmax 中间值），但换来约 3% 左右额外性能提升（FP16 前向从 620 提升到约 640+ TFLOPs）。\n\n\n### **4.2 Hopper 硬件特性利用**\n\n\nFlashAttention-3 针对 H100 的新指令做了专门优化。例如使用 **WGMMA** 指令批量执行 Warp 组矩阵乘法，提高单 Warp 组算力利用；利用 **TMA** 硬件在后台异步搬运数据块，从 HBM 到共享内存，隐藏内存延迟。通过这些手段，在不增加显式同步的情况下，实现了计算和传输高度重叠。另外，FlashAttention-3 使用 NVIDIA 提供的 CUTLASS 库的抽象封装这些操作，加快开发同时保障性能。这些低级优化共同使得FlashAttention-3 在 H100 上达到了**接近峰值**的性能：FP16/BF16 模式下前向最高约 740 TFLOPs（75% 理论峰值），比 FlashAttention-2 在 A100 上的 124 TFLOPs 大幅提升；使用 FP8 精度时则进一步达到 **1.2 PFLOPs** 的惊人水平。\n\n\n### **4.3 低精度 FP8 与数值优化**\n\n\n除了速度，FlashAttention-3 还探索了**更低精度计算**以提高效率。FP8 精度下 Tensor Core 吞吐可翻倍，但直接将注意力降到 FP8 会引入较大数值误差。模型激活常出现“outlier”离群值，用低位表示会造成严重量化误差。为此，FlashAttention-3 引入了**失相干处理（incoherent processing）技术：对** $Q,K$ **每个 head 乘以一个随机正交矩阵（如 Hadamard 变换），将少数大值“扩散”到各维度。这样做可降低量化误差，而且 Hadamard 变换本身是线性操作，可以与其他操作（如 RoPE 位置编码）融合而几乎无额外代价。实验表明，对于模拟含 0.1% 大幅值的输入，失相干处理将 FP8 量化误差降低了2.6倍**。借助对 outlier 的特殊处理，FlashAttention-3 在 FP8 模式下达到与 FP16 几乎相当的准确率损失：RMSE 误差仅为普通 FP8 实现的约 1/2.6。因此，FlashAttention-3 **在不牺牲准确率的前提下**成功利用 FP8 把速度再次提升约 1.3 倍。\n\n\n综合来说，FlashAttention-3 针对**新硬件的异步并行**和**低精度技巧**实现了显著性能突破。在 H100 上，其 FP16 推理和训练速度比 FlashAttention-2 提高约 1.5～2 倍，达到 ~75% 理论 FLOPs 利用；FP8 模式下更是达到了近 1.2 PFLOPs 的前所未有速度。这些优化证明：充分发掘硬件并行特性和结合算法创新，仍能在 Transformer 这样成熟的算子上取得大幅改进。这也为未来进一步优化 LLM 推理、以及在其它硬件上移植类似技术指明了方向。\n\n\n---\n\n\n## 五、Flash-Decoding：自回归长序列解码的并行优化\n\n\n上述 FlashAttention 系列主要针对**训练**场景的长序列优化。然而在**推理/解码**阶段，Transformer 通常是**自回归**地一个一个地生成 token，每次只计算一步注意力。典型情况下，解码时当前查询长度为 1（即每次只生成一个新 token），但需要和前面可能数千甚至数万长度的上下文计算注意力。这一特点导致原版 FlashAttention 在推理时遇到新的性能挑战：\n\n- **低并行度问题：** 自回归解码时每次只有一个查询向量，FlashAttention V1/V2 将线程块并行在查询长度和 batch 上。如果 batch size 也很小（常见于单句生成），那么 GPU 的大部分 SM 在计算一个注意力头时都是闲置的（例如 A100 有 108 个SM，而 batch=1 时 FlashAttention 只用到 1 个 SM 不到）。即使序列很长，$K,V$ 很多，V1/V2 也只能在一两个 SM 上顺序分块处理，**GPU 利用率极低**。\n- **注意力仍是主要瓶颈：** 在推理阶段，Transformer 的其余计算（前馈网络等）可以缓存和批处理，但**注意力的计算量随上下文长度增长**。特别是当支持超长上下文（数万 token）时，注意力占据生成过程的大部分时间。为提升吞吐，必须针对这种“一对多”的特殊情况优化注意力 kernel。\n\n为此，Tri Dao 等人提出了 **Flash-Decoding** 技术。它借鉴 FlashAttention 的思路，但新增**沿 KV 长度的并行**来充分利用 GPU，即**对键/值序列进行拆分并行处理**。核心思想如下：\n\n1. **切分 KV 缓存：**将全部过往 Tokens 的键、值 $K, V$ 矩阵按序列长度方向分割成若干较小的**块（chunk）**。例如总长度 $N$ 分成 $M$ 个 chunk，每个大小约 $N/M$。这些块仅是对原 $K,V$ 在内存中的视图，不需要真实拷贝。\n2. **并行计算局部注意力：**为每个 chunk 启动一个 FlashAttention 内核，计算**当前查询与该 chunk** 的注意力输出**部分** $O^{(j)}$，同时计算该 chunk 局部的 $\\text{log-sum-exp}$ 值（对应 Softmax 分母的一部分）并存储。这一步相当于并行执行多次“查询与子序列”的注意力，产生各自归一化的局部结果和一个缩放系数。\n3. **跨块归并输出：**在上述并行计算完成后，再启动一个小 kernel 将各 chunk 的部分输出 $O^{(j)}$ 合并成完整输出 。合并时利用每个 chunk 提供的 $\\log\\text{-sum-exp}$ 信息，按概率正确加权叠加各部分。这相当于执行一次**全局 Softmax 归一化**：把各块之前局部 Softmax 得到的值按照它们占全局分母的比例进行缩放和累加。\n\n通过上述过程，Flash-Decoding 实现了**两级在线 Softmax**：每个 chunk 内部用了 FlashAttention 的在线算法计算局部 Softmax，chunk 之间再通过一次归并完成全局 Softmax。关键在于，第二步的多 kernel 完全并行使得 GPU 所有 SM 都被利用来处理不同段的 $K,V$。只要上下文长度足够大划分出足够 chunk，即使 batch=1，GPU 也可以**满载运行**注意力计算。这使得推理时注意力耗时基本只随显存带宽线性增长，而不像以前随序列长度急剧恶化。\n\n\n实验显示，在如 Code Llama-34B 等模型上，Flash-Decoding 对长序列（比如 64k tokens）推理速度有**量级提升**：与标准 PyTorch 或 FasterTransformer 等方案相比，长上下文下吞吐最高提升可达 **8 倍**。并且它几乎实现了“横向扩展”：序列长度从 512 增加到 64k，Flash-Decoding 方案的生成速度几乎不受影响，而传统注意力方法速度随长度显著下降。值得强调的是，这种加速**完全不影响输出结果**，仍然是精确的注意力计算，只是巧妙利用了并行资源。Flash-Decoding 已集成进 FlashAttention 库的推理接口（v2.2 版起），为长上下文模型的实际部署提供了实用方案。\n\n\n---\n\n\n## 六、FlashDecoding++：进一步降低生成延迟的优化\n\n\n在 Flash-Decoding 基础上，学界又提出了 **FlashDecoding++**（MLSys 2024），该工作由清华大学等团队完成，着重优化了**推理延迟和跨硬件适配**。FlashDecoding++ 在保持 Flash-Decoding 并行框架的同时，引入三项新技术：\n\n- **异步 Softmax 管线（Asynchronized Softmax）与统一最大值：** 针对 Flash-Decoding 归并各 chunk Softmax 时需要同步等待的问题，FlashDecoding++ 提出**统一最大值**技巧，消除不同部分 Softmax 在归一化时的同步依赖。通过在各部分 Softmax 时引用同一个全局最大值来调整，Softmax 计算可以更早进行流水，减少阻塞。配合**细粒度流水线**调度，作者报告在预填充阶段加速1.18×，解码阶段加速1.14×。\n- **平坦 GEMM 优化与双缓冲：** 生成过程中，小批量时会出现很多不同形状的小矩阵乘（如维度不匹配的 $Q*K$、以及一列一列增大的投影矩阵等），这些**非均匀 GEMM** 难以被统一优化。FlashDecoding++ 分析了这些 GEMM 瓶颈，引入双缓冲（double buffering）等技术，使得 GPU 在执行一个 GEMM 时预取下一个，隐藏内存延迟。针对某些“扁平”大矩阵乘（flat GEMM）的特殊优化带来了高达 52% 的 GEMM 加速。\n- **启发式数据流调度：** 考虑不同硬件（如 NVIDIA Tensor Core vs AMD 矩阵核心）和不同输入特征下，**静态统一的数据流**未必最优。FlashDecoding++ 通过**启发式策略**自适应选择计算走向：例如对小矩阵选用标量核心执行、对大矩阵用 Tensor Core，或对不同 batch /长度采用不同并行度。这种**硬件资源自适应**的数据流让各种场景下都接近最优，作者报告相比固定策略最多提升 29% 速度。\n\n综合以上优化，FlashDecoding++ 展现出**显著的端到端性能提升**：在 NVIDIA A100 和 AMD MI210 上对主流 LLM 模型推理实现了**平均 1.37× 速度提升。尤其在首token延迟**和**流式生成每个 token 延迟**上，FlashDecoding++ 均领先。\n\n\n---\n\n\n## 七、总结：FlashAttention 演进路径与应用场景\n\n\n从 FlashAttention V1 到 V3，以及针对推理的 Flash-Decoding 系列，注意力加速技术完成了从 **IO 优化**到**并行极限**、从**通用训练**到**专门推理**的逐步演进：\n\n- **FlashAttention V1** 聚焦于**算法重组和内存层次优化**，通过块式计算和在线 Softmax 使注意力计算在 GPU 上更加“IO 友好”，实现了注意力精确计算的首次大幅加速。它适用于几乎所有 Transformer 注意力计算场景，大幅降低了显存占用并提升训练速度，已经成为众多开源模型的默认实现。\n- **FlashAttention V2** 在 V1 基础上深入 **GPU 并行架构**优化，解决了长序列小批量时的扩展性问题，引入跨序列并行和 warp 级无同步分工，实现了接近硬件上限的效率。它使得**超长上下文训练**成为可能（例如 2 倍序列长度下仍可高效训练），并在 A100 上达到了与 GEMM 相当级别的高利用。V2 适用于训练和推理中**序列长度极长**但需要充分利用硬件的情况。\n- **FlashAttention V3** 利用**新硬件特性**（如 H100 的异步 Tensor Core、FP8）将注意力计算推进到**流水线并行的新阶段**，显著提升了 Hopper GPU 上的性能上限。通过异步重叠计算和低精度优化，V3 为**下一代硬件**上的 LLM 训练和推理奠定了基础。其 Warp 级流水线思想也可为其他 GPU 程序优化提供思路。\n- **Flash-Decoding 系列**专门面向**自回归推理**这一新挑战，通过**并行化 attention 计算**和一系列 pipeline 改进，将长上下文生成的**端到端延迟显著降低**。Flash-Decoding 让长度从几千提升到数万时的推理成本不再高不可攀，对于需要长文档处理、对话持续上下文等应用非常关键。而FlashDecoding++ 进一步优化了**单步延迟**和**跨硬件性能**，在工业部署中具有吸引力。\n\n总之，FlashAttention 从 V1 到 V3，以及针对解码的扩展，一步步攻克了 Transformer 注意力在**存储、并行、延迟**上的瓶颈，成为 Transformer 加速领域的重要里程碑。对于工程实践者来说，这些算法既提供了**开箱即用**的性能提升，也展现了**贴合硬件特性**进行深度优化的范例。在未来，我们有理由期待这些思路进一步推广到更多模型组件和硬件平台，继续突破大模型训练与推理的效率极限。\n\n\n---\n\n\n## 参考链接\n\n- From Online Softmax to FlashAttention\n\n    [https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf](https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf)\n\n- Online Softmax to Flash Attention — and Why it Matters | by Matthew Gunton | Data Science Collective | Medium\n\n    [https://medium.com/data-science-collective/online-softmax-to-flash-attention-and-why-it-matters-9d676e7c50a8](https://medium.com/data-science-collective/online-softmax-to-flash-attention-and-why-it-matters-9d676e7c50a8)\n\n- FlashAttention 2: making Transformers 800% faster w/o approximation - with Tri Dao of Together AI\n\n    [https://www.latent.space/p/flashattention](https://www.latent.space/p/flashattention)\n\n- FlashAttention by hand - DEV Community\n\n    [https://dev.to/lewis_won/flashattention-by-hand-34im](https://dev.to/lewis_won/flashattention-by-hand-34im)\n\n- Aman's AI Journal • Primers • FlashAttention\n\n    [https://aman.ai/primers/ai/flashattention/](https://aman.ai/primers/ai/flashattention/)\n\n- FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision | Tri Dao\n\n    [https://tridao.me/blog/2024/flash3/](https://tridao.me/blog/2024/flash3/)\n\n- **Flash-Decoding for long-context inference**\n\n    [https://crfm.stanford.edu/2023/10/12/flashdecoding.html](https://crfm.stanford.edu/2023/10/12/flashdecoding.html)\n\n- FlashDecoding++: Faster Large Language Model Inference with Asynchronization, Flat GEMM Optimization, and Heuristics\n\n    [https://proceedings.mlsys.org/paper_files/paper/2024/file/5321b1dabcd2be188d796c21b733e8c7-Paper-Conference.pdf](https://proceedings.mlsys.org/paper_files/paper/2024/file/5321b1dabcd2be188d796c21b733e8c7-Paper-Conference.pdf)","src/content/blog/notion/flashattention.md","91673a9208459c97",{"html":87,"metadata":88},"\u003Chr>\n\u003Ch2 id=\"学习链接\">学习链接\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Ca href=\"https://zhuanlan.zhihu.com/p/669926191\">图解大模型计算加速系列：FlashAttention V1，从硬件到计算逻辑 - 知乎\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://zhuanlan.zhihu.com/p/691067658\">图解大模型计算加速系列：FlashAttention V2，从原理到并行计算 - 知乎\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://zhuanlan.zhihu.com/p/668888063?share_code=15umGexBTQvYV&#x26;utm_psn=1985738448250896443\">[Attention优化][2w字]📚原理篇: 从Online-Softmax到FlashAttention V1/V2/V3 - 知乎\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://arxiv.org/pdf/2307.08691\">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://zhuanlan.zhihu.com/p/1951775373198091592?share_code=1j8zn2LiuCsMG&#x26;utm_psn=1984332043031713739\">flash attention v1-v3系列论文解读[all] - 知乎\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://mp.weixin.qq.com/s/IBeBHO5WlS5BfyL0nZaDHg\">CUDA-MODE课程笔记 第12课，Flash Attention\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://alexdremov.me/understanding-flash-attention-writing-the-algorithm-from-scratch-in-triton/\">Understanding Flash Attention: Writing Triton Kernel Code\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad\">ELI5: FlashAttention. Step by step explanation of how one of… | by Aleksa Gordić | Medium\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://zhuanlan.zhihu.com/p/1973861893090321085?share_code=16ebSp1cEFl2g&#x26;utm_psn=1978981878536086595\">CUDA 学习：FlashAttention-2 (Part 1) - 知乎\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://zhuanlan.zhihu.com/p/1975300779821773120?share_code=rxemUGRPa6M8&#x26;utm_psn=1975339998430638923\">Flash Attention深度解析 - 知乎\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://zhuanlan.zhihu.com/p/1953761827025584899?share_code=EaB9P4s0GMg6&#x26;utm_psn=1953996076680999196\">Flash Attention 全解析（上）：从 V1、V2 到 Flash Decoding 的演进与思想 - 知乎\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://zhuanlan.zhihu.com/p/1954119871420933465\">Flash Attention 全解析(下)：从硬件、算法到指令，榨干 H100 的异步Flash Attention V3 - 知乎\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://zhuanlan.zhihu.com/p/696075602\">[Decoding优化]🔥原理&#x26;图解FlashDecoding/FlashDecoding++ - 知乎\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://zhuanlan.zhihu.com/p/18986650584\">FlashAttention V3 论文原理分析 - 知乎\u003C/a>\u003C/li>\n\u003Cli>\u003Ca href=\"https://zhuanlan.zhihu.com/p/697515825?share_code=10fP0tgWgdStv&#x26;utm_psn=1938149751125161161\">flash attention的CUDA实现探讨-V3 - 知乎\u003C/a>\u003C/li>\n\u003C/ul>\n\u003Chr>\n\u003Ch2 id=\"一引言attention-机制与内存瓶颈背景\">一、引言：Attention 机制与内存瓶颈背景\u003C/h2>\n\u003Cp>自 Transformer 提出以来，“自注意力（Self-Attention）”已经成为大型模型的核心组件。其基本公式为：\u003C/p>\n\u003Cp>\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>O\u003C/mi>\u003Cmo>=\u003C/mo>\u003Cmtext>softmax\u003C/mtext>\u003Cmrow>\u003Cmo fence=\"true\">(\u003C/mo>\u003Cmfrac>\u003Cmrow>\u003Cmi>Q\u003C/mi>\u003Cmsup>\u003Cmi>K\u003C/mi>\u003Cmi>T\u003C/mi>\u003C/msup>\u003C/mrow>\u003Cmsqrt>\u003Cmi>d\u003C/mi>\u003C/msqrt>\u003C/mfrac>\u003Cmo fence=\"true\">)\u003C/mo>\u003C/mrow>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">O = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d}}\\right) V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.8em;vertical-align:-0.65em;\">\u003C/span>\u003Cspan class=\"mord text\">\u003Cspan class=\"mord\">softmax\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"minner\">\u003Cspan class=\"mopen delimcenter\" style=\"top:0em;\">\u003Cspan class=\"delimsizing size2\">(\u003C/span>\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mopen nulldelimiter\">\u003C/span>\u003Cspan class=\"mfrac\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.0895em;\">\u003Cspan style=\"top:-2.5335em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord sqrt mtight\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.9378em;\">\u003Cspan class=\"svg-align\" style=\"top:-3em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"mord mtight\" style=\"padding-left:0.833em;\">\u003Cspan class=\"mord mathnormal mtight\">d\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-2.8978em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"hide-tail mtight\" style=\"min-width:0.853em;height:1.08em;\">\u003Csvg xmlns=\"http://www.w3.org/2000/svg\" width=\"400em\" height=\"1.08em\" viewBox=\"0 0 400000 1080\" preserveAspectRatio=\"xMinYMin slice\">\u003Cpath d=\"M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z\">\u003C/path>\u003C/svg>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.1022em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-3.23em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"frac-line\" style=\"border-bottom-width:0.04em;\">\u003C/span>\u003C/span>\u003Cspan style=\"top:-3.4461em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">Q\u003C/span>\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.9191em;\">\u003Cspan style=\"top:-2.931em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.538em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose nulldelimiter\">\u003C/span>\u003C/span>\u003Cspan class=\"mclose delimcenter\" style=\"top:0em;\">\u003Cspan class=\"delimsizing size2\">)\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/p>\n\u003Cp>其中 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>Q\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>K\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">Q, K, V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span> 分别是查询、键、值矩阵，\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>d\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">d\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">d\u003C/span>\u003C/span>\u003C/span>\u003C/span> 是每个注意力头的维度。这个计算需要先计算 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>Q\u003C/mi>\u003Cmsup>\u003Cmi>K\u003C/mi>\u003Cmi>T\u003C/mi>\u003C/msup>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">QK^T\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0358em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8413em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span> 得到 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>N\u003C/mi>\u003Cmo>×\u003C/mo>\u003Cmi>N\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">N \\times N\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003Cspan class=\"mbin\">×\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N\u003C/span>\u003C/span>\u003C/span>\u003C/span> 的注意力得分矩阵，再对每一行执行 Softmax 归一化，最后与 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span> 相乘得到输出 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>O\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">O\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O\u003C/span>\u003C/span>\u003C/span>\u003C/span>。如图 1 所示，标准实现中，这三个步骤通常拆分为独立的矩阵运算，会产生大量中间结果。\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/notion/1fb22dca-4210-80cd-a96e-e32787cfd674/2d022dca-4210-80ec-a195-c3adbd923096.png\" alt=\"图 1\">\u003C/p>\n\u003Cp>\u003Cstrong>内存与计算挑战：\u003C/strong> 注意力的时间和空间复杂度均为 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>O\u003C/mi>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmsup>\u003Cmi>N\u003C/mi>\u003Cmn>2\u003C/mn>\u003C/msup>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">O(N^2)\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8141em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">2\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>，当序列长度 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>N\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">N\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N\u003C/span>\u003C/span>\u003C/span>\u003C/span> 增大时，内存占用和数据传输量会呈二次方增长。例如，长度翻倍会导致注意力矩阵元素数量增加四倍。这导致 GPU 上\u003Cstrong>内存访问\u003C/strong>成为瓶颈——对巨大的 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>Q\u003C/mi>\u003Cmsup>\u003Cmi>K\u003C/mi>\u003Cmi>T\u003C/mi>\u003C/msup>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">QK^T\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0358em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8413em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span> 矩阵和 Softmax 中间结果的反复读写使计算受限于内存带宽。事实上，在大模型推理中，尽管 GPU 算力很强，\u003Cstrong>显存的读写速度\u003C/strong>往往限制了注意力层的性能。\u003C/p>\n\u003Cp>此外，Softmax 计算本身也存在\u003Cstrong>数值稳定\u003C/strong>问题：直接对大数取指数可能溢出，需要减去最大值来稳定计算，这通常需要多次遍历数据（例如“三遍法”分别计算最大值、指数和归一化）。总的来说，\u003Cstrong>注意力的内存开销和 Softmax 处理\u003C/strong>成为 Transformer 扩展长序列时亟待解决的瓶颈。\u003C/p>\n\u003Cp>FlashAttention 正是在这种背景下诞生：通过\u003Cstrong>重排计算顺序和融合算子\u003C/strong>，以 \u003Cstrong>IO（内存访问）优化\u003C/strong>为核心，使注意力计算的内存占用从二次降到线性，并显著提升实际运行速度。下文将详细介绍 FlashAttention 的各个版本（V1、V2、V3）的算法原理与实现优化。\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"二flashattention-v1块式算法与-online-softmax-优化\">二、FlashAttention V1：块式算法与 Online Softmax 优化\u003C/h2>\n\u003Cp>FlashAttention V1（最初发表于 2022 年）是 Tri Dao 等人提出的\u003Cstrong>精确且内存高效\u003C/strong>的注意力计算算法。它的核心思想是在\u003Cstrong>不引入近似\u003C/strong>的前提下，通过\u003Cstrong>块式（block-wise）计算\u003C/strong>和\u003Cstrong>在线 Softmax\u003C/strong> 技巧，将注意力的中间结果限制在高速缓存中，从而减少显存读写。\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/notion/1fb22dca-4210-80cd-a96e-e32787cfd674/2d222dca-4210-808b-8f4d-e92e18a8d8b6.png\" alt=\"\">\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/notion/1fb22dca-4210-80cd-a96e-e32787cfd674/2cf22dca-4210-801c-9beb-dddea5d7e160.png\" alt=\"图 2：FlashAttention V1 的块式注意力计算原理示意图。将长序列的 K, V 拆分成多个块分批加载至片上高速内存（SRAM）计算，与每个块对应的 Q 批次进行乘积并软最大归一化，再累积输出结果。蓝色框表示存储在 GPU 显存（HBM）的大矩阵未被 materialize，橙色虚线框表示在片上 SRAM 中计算的部分。最终通过重新缩放确保整体 Softmax 正确归一化。\">\u003C/p>\n\u003Ch3 id=\"21-块式计算与内存优化\">2.1 块式计算与内存优化\u003C/h3>\n\u003Cp>\u003Cstrong>块式（tiling）计算：\u003C/strong> FlashAttention 将 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>Q\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>K\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">Q, K, V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span> 划分为适合片上缓存的小块，逐块完成注意力计算。具体而言，算法每次从显存（HBM）加载一块 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>K\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">K\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003C/span>\u003C/span>\u003C/span> 和对应的 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span>（例如块大小 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>B\u003C/mi>\u003Cmo>×\u003C/mo>\u003Cmi>d\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">B \\times d\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003Cspan class=\"mbin\">×\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">d\u003C/span>\u003C/span>\u003C/span>\u003C/span>）到共享内存，然后对这块数据与所有查询 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>Q\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">Q\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003C/span>\u003C/span>\u003C/span> 分块进行计算。与传统方法一次性计算整个 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>Q\u003C/mi>\u003Cmsup>\u003Cmi>K\u003C/mi>\u003Cmi>T\u003C/mi>\u003C/msup>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">QK^T\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0358em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8413em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span> 不同，FlashAttention \u003Cstrong>从不在显存中构造完整的\u003C/strong> \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>N\u003C/mi>\u003Cmo>×\u003C/mo>\u003Cmi>N\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">N \\times N\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003Cspan class=\"mbin\">×\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N\u003C/span>\u003C/span>\u003C/span>\u003C/span> \u003Cstrong>注意力矩阵\u003C/strong>。取而代之，它在片上 SRAM 中\u003Cstrong>分块计算并及时消耗\u003C/strong>这些局部结果。例如，假设将 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>K\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">K,V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span> 各分成两块：先计算第一块 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsup>\u003Cmi>K\u003C/mi>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmn>1\u003C/mn>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003C/msup>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmsup>\u003Cmi>V\u003C/mi>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmn>1\u003C/mn>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003C/msup>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">K^{(1)}, V^{(1)}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0824em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.888em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mopen mtight\">(\u003C/span>\u003Cspan class=\"mord mtight\">1\u003C/span>\u003Cspan class=\"mclose mtight\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.888em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mopen mtight\">(\u003C/span>\u003Cspan class=\"mord mtight\">1\u003C/span>\u003Cspan class=\"mclose mtight\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span> 对输出的贡献，再计算第二块 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsup>\u003Cmi>K\u003C/mi>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmn>2\u003C/mn>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003C/msup>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmsup>\u003Cmi>V\u003C/mi>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmn>2\u003C/mn>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003C/msup>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">K^{(2)}, V^{(2)}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0824em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.888em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mopen mtight\">(\u003C/span>\u003Cspan class=\"mord mtight\">2\u003C/span>\u003Cspan class=\"mclose mtight\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.888em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mopen mtight\">(\u003C/span>\u003Cspan class=\"mord mtight\">2\u003C/span>\u003Cspan class=\"mclose mtight\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>，最后将两部分结果正确叠加。由于\u003Cstrong>不写入/读取大中间矩阵\u003C/strong>，这种分块策略使所需内存读写量从 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>O\u003C/mi>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmsup>\u003Cmi>N\u003C/mi>\u003Cmn>2\u003C/mn>\u003C/msup>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">O(N^2)\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8141em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">2\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span> 降至 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>O\u003C/mi>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>N\u003C/mi>\u003Cmo>⋅\u003C/mo>\u003Cmi>d\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">O(N \\cdot d)\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003Cspan class=\"mbin\">⋅\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">d\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>，大幅降低了GPU全局内存访问。实践中，这种 IO 优化带来了约 \u003Cstrong>2～4倍\u003C/strong> 的时间加速。更重要的是，\u003Cstrong>内存占用从二次降为线性\u003C/strong>，使得长序列（如数万长度）在有限显存中成为可能。\u003C/p>\n\u003Cp>\u003Cstrong>融合与单Kernel实现：\u003C/strong> FlashAttention 将注意力的多步计算融合进\u003Cstrong>单个 CUDA 核函数\u003C/strong>。传统实现需要依次启动独立 kernel 计算 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>Q\u003C/mi>\u003Cmsup>\u003Cmi>K\u003C/mi>\u003Cmi>T\u003C/mi>\u003C/msup>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">QK^T\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0358em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8413em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>、Softmax、\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>A\u003C/mi>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">AV\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">A\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span>，期间中间结果多次写回显存。FlashAttention V1 则通过一个内核完成\u003Cstrong>点积、Softmax 归一化、再乘\u003C/strong> \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span> \u003Cstrong>的完整流程，并处理必要的 mask 操作。这消除了内核间切换和同步开销，也避免了不同 kernel 之间反复读写显存。这一“kernel 融合”手段结合块式计算，使得注意力的计算完全在片上缓存中进行，大幅减少了慢速 HBM 的访问。总之，FlashAttention V1 充分利用 GPU 内存层次结构（寄存器、共享内存 vs. 全局显存），通过以算代存\u003C/strong>来提升性能——牺牲一些额外的算术操作来避免昂贵的内存 I/O。\u003C/p>\n\u003Ch3 id=\"22-online-softmax把全行归一化拆成逐块可合并的稳定统计量\">2.2 Online Softmax：把“全行归一化”拆成“逐块可合并”的稳定统计量\u003C/h3>\n\u003Cp>块式 attention 的最大障碍在 Softmax：对每个 query 行 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>i\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">i\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6595em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>，Softmax 需要整行 logits \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsub>\u003Cmi>S\u003C/mi>\u003Cmi>i\u003C/mi>\u003C/msub>\u003Cmo>=\u003C/mo>\u003Cmsub>\u003Cmi>Q\u003C/mi>\u003Cmi>i\u003C/mi>\u003C/msub>\u003Cmsup>\u003Cmi>K\u003C/mi>\u003Cmi mathvariant=\"normal\">⊤\u003C/mi>\u003C/msup>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">S_i = Q_i K^\\top\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3117em;\">\u003Cspan style=\"top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0435em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3117em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8491em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">⊤\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span> 的 \u003Cstrong>全局最大值\u003C/strong>与\u003Cstrong>全局指数和\u003C/strong>：\u003C/p>\n\u003Cp>\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmrow>\u003Cmi mathvariant=\"normal\">s\u003C/mi>\u003Cmi mathvariant=\"normal\">o\u003C/mi>\u003Cmi mathvariant=\"normal\">f\u003C/mi>\u003Cmi mathvariant=\"normal\">t\u003C/mi>\u003Cmi mathvariant=\"normal\">m\u003C/mi>\u003Cmi mathvariant=\"normal\">a\u003C/mi>\u003Cmi mathvariant=\"normal\">x\u003C/mi>\u003C/mrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmsub>\u003Cmi>S\u003C/mi>\u003Cmi>i\u003C/mi>\u003C/msub>\u003Cmsub>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003Cmi>t\u003C/mi>\u003C/msub>\u003Cmo>=\u003C/mo>\u003Cmfrac>\u003Cmsup>\u003Cmi>e\u003C/mi>\u003Cmsub>\u003Cmi>S\u003C/mi>\u003Cmrow>\u003Cmi>i\u003C/mi>\u003Cmi>t\u003C/mi>\u003C/mrow>\u003C/msub>\u003C/msup>\u003Cmrow>\u003Cmsub>\u003Cmo>∑\u003C/mo>\u003Cmi>u\u003C/mi>\u003C/msub>\u003Cmsup>\u003Cmi>e\u003C/mi>\u003Cmsub>\u003Cmi>S\u003C/mi>\u003Cmrow>\u003Cmi>i\u003C/mi>\u003Cmi>u\u003C/mi>\u003C/mrow>\u003C/msub>\u003C/msup>\u003C/mrow>\u003C/mfrac>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\mathrm{softmax}(S_i)_t = \\frac{e^{S_{it}}}{\\sum_{u} e^{S_{iu}}}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathrm\">softmax\u003C/span>\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3117em;\">\u003Cspan style=\"top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose\">\u003Cspan class=\"mclose\">)\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.2806em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">t\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.6468em;vertical-align:-0.6095em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mopen nulldelimiter\">\u003C/span>\u003Cspan class=\"mfrac\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.0374em;\">\u003Cspan style=\"top:-2.6156em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mop mtight\">\u003Cspan class=\"mop op-symbol small-op mtight\" style=\"position:relative;top:0em;\">∑\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:-0.0139em;\">\u003Cspan style=\"top:-2.1786em;margin-left:0em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">u\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3214em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace mtight\" style=\"margin-right:0.1952em;\">\u003C/span>\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">e\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8206em;\">\u003Cspan style=\"top:-2.8326em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05764em;\">S\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3448em;\">\u003Cspan style=\"top:-2.3448em;margin-left:-0.0576em;margin-right:0.1em;\">\u003Cspan class=\"pstrut\" style=\"height:2.6595em;\">\u003C/span>\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003Cspan class=\"mord mathnormal mtight\">u\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3147em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-3.23em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"frac-line\" style=\"border-bottom-width:0.04em;\">\u003C/span>\u003C/span>\u003Cspan style=\"top:-3.394em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">e\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.9191em;\">\u003Cspan style=\"top:-2.931em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05764em;\">S\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3448em;\">\u003Cspan style=\"top:-2.3448em;margin-left:-0.0576em;margin-right:0.1em;\">\u003Cspan class=\"pstrut\" style=\"height:2.6595em;\">\u003C/span>\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003Cspan class=\"mord mathnormal mtight\">t\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3147em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.6095em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose nulldelimiter\">\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/p>\n\u003Cp>如果我们把 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>K\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">K,V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span> 沿序列维分成多个块（block），那么每次只看到 logits 的一个子向量 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsubsup>\u003Cmi>S\u003C/mi>\u003Cmi>i\u003C/mi>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>j\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003C/msubsup>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">S_i^{(j)}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.3217em;vertical-align:-0.2769em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.0448em;\">\u003Cspan style=\"top:-2.4231em;margin-left:-0.0576em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-3.2198em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mopen mtight\">(\u003C/span>\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j\u003C/span>\u003Cspan class=\"mclose mtight\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.2769em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>。想要不 materialize 全量 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsub>\u003Cmi>S\u003C/mi>\u003Cmi>i\u003C/mi>\u003C/msub>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">S_i\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3117em;\">\u003Cspan style=\"top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>，就必须支持“\u003Cstrong>看到一块就更新一次 Softmax 的归一化信息\u003C/strong>”。\u003C/p>\n\u003Cp>FlashAttention 的做法是：为每个 query 行维护两类 \u003Cstrong>可增量合并\u003C/strong> 的稳定统计量：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsub>\u003Cmi>m\u003C/mi>\u003Cmi>i\u003C/mi>\u003C/msub>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">m_i\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">m\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3117em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>：到目前为止见过的 logits 的\u003Cstrong>运行最大值\u003C/strong>（running max）\u003C/li>\n\u003Cli>\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsub>\u003Cmi mathvariant=\"normal\">ℓ\u003C/mi>\u003Cmi>i\u003C/mi>\u003C/msub>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\ell_i\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8444em;vertical-align:-0.15em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord\">ℓ\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3117em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>：到目前为止见过的 logits 的\u003Cstrong>稳定指数和\u003C/strong>（running exp-sum），也可以理解为 ⁡\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>exp\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\exp\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mop\">exp\u003C/span>\u003C/span>\u003C/span>\u003C/span> 空间的分母，但总是以 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmo>⋅\u003C/mo>\u003Cmo>−\u003C/mo>\u003Cmsub>\u003Cmi>m\u003C/mi>\u003Cmi>i\u003C/mi>\u003C/msub>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">(\\cdot - m_i)\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">⋅\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003Cspan class=\"mbin\">−\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">m\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3117em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span> 的形式存储\u003C/li>\n\u003C/ul>\n\u003Cp>这对应经典 log-sum-exp（LSE）稳定形式：\u003C/p>\n\u003Cspan class=\"katex-display\">\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\u003Csemantics>\u003Cmrow>\u003Cmi>m\u003C/mi>\u003Cmo>=\u003C/mo>\u003Cmunder>\u003Cmrow>\u003Cmi>max\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003C/mrow>\u003Cmi>t\u003C/mi>\u003C/munder>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmi>t\u003C/mi>\u003C/msub>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmspace width=\"1em\">\u003C/mspace>\u003Cmi mathvariant=\"normal\">ℓ\u003C/mi>\u003Cmo>=\u003C/mo>\u003Cmunder>\u003Cmo>∑\u003C/mo>\u003Cmi>t\u003C/mi>\u003C/munder>\u003Cmsup>\u003Cmi>e\u003C/mi>\u003Cmrow>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmi>t\u003C/mi>\u003C/msub>\u003Cmo>−\u003C/mo>\u003Cmi>m\u003C/mi>\u003C/mrow>\u003C/msup>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmspace width=\"1em\">\u003C/mspace>\u003Cmi>log\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003Cmunder>\u003Cmo>∑\u003C/mo>\u003Cmi>t\u003C/mi>\u003C/munder>\u003Cmsup>\u003Cmi>e\u003C/mi>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmi>t\u003C/mi>\u003C/msub>\u003C/msup>\u003Cmo>=\u003C/mo>\u003Cmi>m\u003C/mi>\u003Cmo>+\u003C/mo>\u003Cmi>log\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003Cmi mathvariant=\"normal\">ℓ\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">m = \\max_t x_t,\\quad\n\\ell = \\sum_t e^{x_t - m},\\quad\n\\log \\sum_t e^{x_t} = m + \\log \\ell\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.4306em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">m\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.3944em;vertical-align:-0.7em;\">\u003C/span>\u003Cspan class=\"mop op-limits\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.4306em;\">\u003Cspan style=\"top:-2.4em;margin-left:0em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">t\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-3em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan>\u003Cspan class=\"mop\">max\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.7em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.2806em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">t\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:1em;\">\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">ℓ\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:2.3em;vertical-align:-1.25em;\">\u003C/span>\u003Cspan class=\"mop op-limits\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.05em;\">\u003Cspan style=\"top:-1.9em;margin-left:0em;\">\u003Cspan class=\"pstrut\" style=\"height:3.05em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">t\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-3.05em;\">\u003Cspan class=\"pstrut\" style=\"height:3.05em;\">\u003C/span>\u003Cspan>\u003Cspan class=\"mop op-symbol large-op\">∑\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.25em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">e\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8213em;\">\u003Cspan style=\"top:-3.113em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.2963em;\">\u003Cspan style=\"top:-2.357em;margin-left:0em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mathnormal mtight\">t\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.143em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mbin mtight\">−\u003C/span>\u003Cspan class=\"mord mathnormal mtight\">m\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:1em;\">\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mop\">lo\u003Cspan style=\"margin-right:0.01389em;\">g\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mop op-limits\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.05em;\">\u003Cspan style=\"top:-1.9em;margin-left:0em;\">\u003Cspan class=\"pstrut\" style=\"height:3.05em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">t\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-3.05em;\">\u003Cspan class=\"pstrut\" style=\"height:3.05em;\">\u003C/span>\u003Cspan>\u003Cspan class=\"mop op-symbol large-op\">∑\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.25em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">e\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.7144em;\">\u003Cspan style=\"top:-3.113em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.2963em;\">\u003Cspan style=\"top:-2.357em;margin-left:0em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mathnormal mtight\">t\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.143em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">m\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003Cspan class=\"mbin\">+\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mop\">lo\u003Cspan style=\"margin-right:0.01389em;\">g\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">ℓ\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\n\u003Cp>假设我们已经处理完前 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>j\u003C/mi>\u003Cmo>−\u003C/mo>\u003Cmn>1\u003C/mn>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">j-1\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.854em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.05724em;\">j\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003Cspan class=\"mbin\">−\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6444em;\">\u003C/span>\u003Cspan class=\"mord\">1\u003C/span>\u003C/span>\u003C/span>\u003C/span> 个块，得到 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmsub>\u003Cmi>m\u003C/mi>\u003Cmtext>old\u003C/mtext>\u003C/msub>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmsub>\u003Cmi mathvariant=\"normal\">ℓ\u003C/mi>\u003Cmtext>old\u003C/mtext>\u003C/msub>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">(m_{\\text{old}}, \\ell_{\\text{old}})\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">m\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3361em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">old\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord\">ℓ\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3361em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">old\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>。现在处理第 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>j\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">j\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.854em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.05724em;\">j\u003C/span>\u003C/span>\u003C/span>\u003C/span> 个块，先算它自己的局部统计量：\u003C/p>\n\u003Cspan class=\"katex-display\">\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\u003Csemantics>\u003Cmrow>\u003Cmsub>\u003Cmi>m\u003C/mi>\u003Cmtext>blk\u003C/mtext>\u003C/msub>\u003Cmo>=\u003C/mo>\u003Cmi>max\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmsup>\u003Cmi>S\u003C/mi>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>j\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003C/msup>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmspace width=\"1em\">\u003C/mspace>\u003Cmsub>\u003Cmi mathvariant=\"normal\">ℓ\u003C/mi>\u003Cmtext>blk\u003C/mtext>\u003C/msub>\u003Cmo>=\u003C/mo>\u003Cmo>∑\u003C/mo>\u003Cmsup>\u003Cmi>e\u003C/mi>\u003Cmrow>\u003Cmsup>\u003Cmi>S\u003C/mi>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>j\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003C/msup>\u003Cmo>−\u003C/mo>\u003Cmsub>\u003Cmi>m\u003C/mi>\u003Cmtext>blk\u003C/mtext>\u003C/msub>\u003C/mrow>\u003C/msup>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">m_{\\text{blk}} = \\max(S^{(j)}),\\quad\n\\ell_{\\text{blk}} = \\sum e^{S^{(j)} - m_{\\text{blk}}}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">m\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3361em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">blk\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.188em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mop\">max\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">S\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.938em;\">\u003Cspan style=\"top:-3.113em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mopen mtight\">(\u003C/span>\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j\u003C/span>\u003Cspan class=\"mclose mtight\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:1em;\">\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord\">ℓ\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3361em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">blk\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.6397em;vertical-align:-0.55em;\">\u003C/span>\u003Cspan class=\"mop op-symbol large-op\" style=\"position:relative;top:0em;\">∑\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">e\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.0897em;\">\u003Cspan style=\"top:-3.113em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05764em;\">S\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.9667em;\">\u003Cspan style=\"top:-2.9667em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5357em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mopen mtight\">(\u003C/span>\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j\u003C/span>\u003Cspan class=\"mclose mtight\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mbin mtight\">−\u003C/span>\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">m\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3448em;\">\u003Cspan style=\"top:-2.3488em;margin-left:0em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">blk\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.1512em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\n\u003Cp>合并时，新全局最大值就是：\u003C/p>\n\u003Cp>\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsub>\u003Cmi>m\u003C/mi>\u003Cmtext>new\u003C/mtext>\u003C/msub>\u003Cmo>=\u003C/mo>\u003Cmi>max\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmsub>\u003Cmi>m\u003C/mi>\u003Cmtext>old\u003C/mtext>\u003C/msub>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmsub>\u003Cmi>m\u003C/mi>\u003Cmtext>blk\u003C/mtext>\u003C/msub>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">m_{\\text{new}}=\\max(m_{\\text{old}}, m_{\\text{blk}})\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">m\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.1514em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">new\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mop\">max\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">m\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3361em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">old\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">m\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3361em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">blk\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/p>\n\u003Cp>关键在于：\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi mathvariant=\"normal\">ℓ\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\ell\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6944em;\">\u003C/span>\u003Cspan class=\"mord\">ℓ\u003C/span>\u003C/span>\u003C/span>\u003C/span> 的基准从旧的 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsub>\u003Cmi>m\u003C/mi>\u003Cmtext>old\u003C/mtext>\u003C/msub>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">m_{\\text{old}}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">m\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3361em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">old\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span> 切换到了新的 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsub>\u003Cmi>m\u003C/mi>\u003Cmtext>new\u003C/mtext>\u003C/msub>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">m_{\\text{new}}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">m\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.1514em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">new\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>，所以要对旧的累积指数和做一次“重标定（rescale）”：\u003C/p>\n\u003Cspan class=\"katex-display\">\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\u003Csemantics>\u003Cmrow>\u003Cmsub>\u003Cmi mathvariant=\"normal\">ℓ\u003C/mi>\u003Cmtext>new\u003C/mtext>\u003C/msub>\u003Cmo>=\u003C/mo>\u003Cmsub>\u003Cmi mathvariant=\"normal\">ℓ\u003C/mi>\u003Cmtext>old\u003C/mtext>\u003C/msub>\u003Cmo>⋅\u003C/mo>\u003Cmsup>\u003Cmi>e\u003C/mi>\u003Cmrow>\u003Cmsub>\u003Cmi>m\u003C/mi>\u003Cmtext>old\u003C/mtext>\u003C/msub>\u003Cmo>−\u003C/mo>\u003Cmsub>\u003Cmi>m\u003C/mi>\u003Cmtext>new\u003C/mtext>\u003C/msub>\u003C/mrow>\u003C/msup>\u003Cmo>+\u003C/mo>\u003Cmsub>\u003Cmi mathvariant=\"normal\">ℓ\u003C/mi>\u003Cmtext>blk\u003C/mtext>\u003C/msub>\u003Cmo>⋅\u003C/mo>\u003Cmsup>\u003Cmi>e\u003C/mi>\u003Cmrow>\u003Cmsub>\u003Cmi>m\u003C/mi>\u003Cmtext>blk\u003C/mtext>\u003C/msub>\u003Cmo>−\u003C/mo>\u003Cmsub>\u003Cmi>m\u003C/mi>\u003Cmtext>new\u003C/mtext>\u003C/msub>\u003C/mrow>\u003C/msup>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\ell_{\\text{new}}\n=\n\\ell_{\\text{old}} \\cdot e^{m_{\\text{old}}-m_{\\text{new}}}\n+\n\\ell_{\\text{blk}} \\cdot e^{m_{\\text{blk}}-m_{\\text{new}}}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8444em;vertical-align:-0.15em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord\">ℓ\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.1514em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">new\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8444em;vertical-align:-0.15em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord\">ℓ\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3361em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">old\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003Cspan class=\"mbin\">⋅\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.9047em;vertical-align:-0.0833em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">e\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8213em;\">\u003Cspan style=\"top:-3.113em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">m\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3448em;\">\u003Cspan style=\"top:-2.3488em;margin-left:0em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">old\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.1512em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mbin mtight\">−\u003C/span>\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">m\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.1645em;\">\u003Cspan style=\"top:-2.357em;margin-left:0em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">new\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.143em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003Cspan class=\"mbin\">+\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8444em;vertical-align:-0.15em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord\">ℓ\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3361em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">blk\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003Cspan class=\"mbin\">⋅\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8213em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">e\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8213em;\">\u003Cspan style=\"top:-3.113em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">m\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3448em;\">\u003Cspan style=\"top:-2.3488em;margin-left:0em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">blk\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.1512em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mbin mtight\">−\u003C/span>\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">m\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.1645em;\">\u003Cspan style=\"top:-2.357em;margin-left:0em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">new\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.143em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\n\u003Cp>这一条就是 Online Softmax 的核心：\u003Cstrong>你不需要保存任何历史 logits，只要保存\u003C/strong> \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>m\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi mathvariant=\"normal\">ℓ\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">(m,\\ell)\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord mathnormal\">m\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">ℓ\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cstrong>，就能把新的块稳定地合并进去。\u003C/strong>\u003C/p>\n\u003Ch3 id=\"23-累积输出把-esvsum-esvesv-也做成可合并的形式\">2.3 累积输出：把 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmo>∑\u003C/mo>\u003Cmsup>\u003Cmi>e\u003C/mi>\u003Cmi>S\u003C/mi>\u003C/msup>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\sum e^{S}V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0913em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">e\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8413em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05764em;\">S\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span> 也做成“可合并”的形式\u003C/h3>\n\u003Cp>Softmax 的最终输出是：\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>O\u003C/mi>\u003Cmo>=\u003C/mo>\u003Cmfrac>\u003Cmrow>\u003Cmsub>\u003Cmo>∑\u003C/mo>\u003Cmi>t\u003C/mi>\u003C/msub>\u003Cmsup>\u003Cmi>e\u003C/mi>\u003Cmsub>\u003Cmi>S\u003C/mi>\u003Cmi>t\u003C/mi>\u003C/msub>\u003C/msup>\u003Cmsub>\u003Cmi>V\u003C/mi>\u003Cmi>t\u003C/mi>\u003C/msub>\u003C/mrow>\u003Cmrow>\u003Cmsub>\u003Cmo>∑\u003C/mo>\u003Cmi>t\u003C/mi>\u003C/msub>\u003Cmsup>\u003Cmi>e\u003C/mi>\u003Cmsub>\u003Cmi>S\u003C/mi>\u003Cmi>t\u003C/mi>\u003C/msub>\u003C/msup>\u003C/mrow>\u003C/mfrac>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">O = \\frac{\\sum_t e^{S_t} V_t}{\\sum_t e^{S_t}}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.7656em;vertical-align:-0.5872em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mopen nulldelimiter\">\u003C/span>\u003Cspan class=\"mfrac\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.1784em;\">\u003Cspan style=\"top:-2.6378em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mop mtight\">\u003Cspan class=\"mop op-symbol small-op mtight\" style=\"position:relative;top:0em;\">∑\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.1179em;\">\u003Cspan style=\"top:-2.1786em;margin-left:0em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mathnormal mtight\">t\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3214em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace mtight\" style=\"margin-right:0.1952em;\">\u003C/span>\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">e\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.7889em;\">\u003Cspan style=\"top:-2.8008em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05764em;\">S\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3448em;\">\u003Cspan style=\"top:-2.3448em;margin-left:-0.0576em;margin-right:0.1em;\">\u003Cspan class=\"pstrut\" style=\"height:2.6151em;\">\u003C/span>\u003Cspan class=\"mord mathnormal mtight\">t\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.2703em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-3.23em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"frac-line\" style=\"border-bottom-width:0.04em;\">\u003C/span>\u003C/span>\u003Cspan style=\"top:-3.535em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mop mtight\">\u003Cspan class=\"mop op-symbol small-op mtight\" style=\"position:relative;top:0em;\">∑\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.1179em;\">\u003Cspan style=\"top:-2.1786em;margin-left:0em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mathnormal mtight\">t\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3214em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace mtight\" style=\"margin-right:0.1952em;\">\u003C/span>\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">e\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.9191em;\">\u003Cspan style=\"top:-2.931em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05764em;\">S\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3448em;\">\u003Cspan style=\"top:-2.3448em;margin-left:-0.0576em;margin-right:0.1em;\">\u003Cspan class=\"pstrut\" style=\"height:2.6151em;\">\u003C/span>\u003Cspan class=\"mord mathnormal mtight\">t\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.2703em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.2963em;\">\u003Cspan style=\"top:-2.357em;margin-left:-0.2222em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mathnormal mtight\">t\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.143em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.5872em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose nulldelimiter\">\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/p>\n\u003Cp>如果我们只更新分母 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi mathvariant=\"normal\">ℓ\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\ell\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6944em;\">\u003C/span>\u003Cspan class=\"mord\">ℓ\u003C/span>\u003C/span>\u003C/span>\u003C/span> 还不够，还得更新分子（加权和值）。FlashAttention 做法是维护一个未归一化的累计分子（但同样用稳定基准）：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>A\u003C/mi>\u003Cmi>c\u003C/mi>\u003Cmi>c\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">Acc\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">A\u003C/span>\u003Cspan class=\"mord mathnormal\">cc\u003C/span>\u003C/span>\u003C/span>\u003C/span>：到目前为止的\u003Cstrong>稳定加权和\u003C/strong>（running weighted sum）\u003C/li>\n\u003C/ul>\n\u003Cp>对第 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>j\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">j\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.854em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.05724em;\">j\u003C/span>\u003C/span>\u003C/span>\u003C/span> 块，先算局部未归一化权重：\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsup>\u003Cmi>P\u003C/mi>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>j\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003C/msup>\u003Cmo>=\u003C/mo>\u003Cmsup>\u003Cmi>e\u003C/mi>\u003Cmrow>\u003Cmsup>\u003Cmi>S\u003C/mi>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>j\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003C/msup>\u003Cmo>−\u003C/mo>\u003Cmsub>\u003Cmi>m\u003C/mi>\u003Cmtext>new\u003C/mtext>\u003C/msub>\u003C/mrow>\u003C/msup>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">P^{(j)} = e^{S^{(j)} - m_{\\text{new}}}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.888em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.888em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mopen mtight\">(\u003C/span>\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j\u003C/span>\u003Cspan class=\"mclose mtight\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0397em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">e\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.0397em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05764em;\">S\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.9667em;\">\u003Cspan style=\"top:-2.9667em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5357em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mopen mtight\">(\u003C/span>\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j\u003C/span>\u003Cspan class=\"mclose mtight\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mbin mtight\">−\u003C/span>\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">m\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.1645em;\">\u003Cspan style=\"top:-2.357em;margin-left:0em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">new\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.143em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>，然后做两件事：\u003C/p>\n\u003Col>\n\u003Cli>把旧的累计分子从基准 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsub>\u003Cmi>m\u003C/mi>\u003Cmtext>old\u003C/mtext>\u003C/msub>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">m_{\\text{old}}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">m\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3361em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">old\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span> 切到 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsub>\u003Cmi>m\u003C/mi>\u003Cmtext>new\u003C/mtext>\u003C/msub>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">m_{\\text{new}}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.5806em;vertical-align:-0.15em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">m\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.1514em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">new\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>（同样 rescale）\u003C/li>\n\u003Cli>加上当前块的贡献 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsup>\u003Cmi>P\u003C/mi>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>j\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003C/msup>\u003Cmsup>\u003Cmi>V\u003C/mi>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>j\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003C/msup>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">P^{(j)}V^{(j)}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.888em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.888em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mopen mtight\">(\u003C/span>\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j\u003C/span>\u003Cspan class=\"mclose mtight\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.888em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mopen mtight\">(\u003C/span>\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j\u003C/span>\u003Cspan class=\"mclose mtight\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/li>\n\u003C/ol>\n\u003Cp>写成公式就是：\u003C/p>\n\u003Cspan class=\"katex-display\">\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\u003Csemantics>\u003Cmrow>\u003Cmi>A\u003C/mi>\u003Cmi>c\u003C/mi>\u003Cmsub>\u003Cmi>c\u003C/mi>\u003Cmtext>new\u003C/mtext>\u003C/msub>\u003Cmo>=\u003C/mo>\u003Cmi>A\u003C/mi>\u003Cmi>c\u003C/mi>\u003Cmsub>\u003Cmi>c\u003C/mi>\u003Cmtext>old\u003C/mtext>\u003C/msub>\u003Cmo>⋅\u003C/mo>\u003Cmsup>\u003Cmi>e\u003C/mi>\u003Cmrow>\u003Cmsub>\u003Cmi>m\u003C/mi>\u003Cmtext>old\u003C/mtext>\u003C/msub>\u003Cmo>−\u003C/mo>\u003Cmsub>\u003Cmi>m\u003C/mi>\u003Cmtext>new\u003C/mtext>\u003C/msub>\u003C/mrow>\u003C/msup>\u003Cmo>+\u003C/mo>\u003Cmrow>\u003Cmo fence=\"true\">(\u003C/mo>\u003Cmsup>\u003Cmi>P\u003C/mi>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>j\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003C/msup>\u003Cmsup>\u003Cmi>V\u003C/mi>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>j\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003C/msup>\u003Cmo fence=\"true\">)\u003C/mo>\u003C/mrow>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">Acc_{\\text{new}}\n=\nAcc_{\\text{old}} \\cdot e^{m_{\\text{old}}-m_{\\text{new}}}\n+\n\\left(P^{(j)} V^{(j)}\\right)\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">A\u003C/span>\u003Cspan class=\"mord mathnormal\">c\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">c\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.1514em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">new\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">A\u003C/span>\u003Cspan class=\"mord mathnormal\">c\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">c\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3361em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">old\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003Cspan class=\"mbin\">⋅\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.9047em;vertical-align:-0.0833em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">e\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8213em;\">\u003Cspan style=\"top:-3.113em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">m\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3448em;\">\u003Cspan style=\"top:-2.3488em;margin-left:0em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">old\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.1512em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mbin mtight\">−\u003C/span>\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">m\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.1645em;\">\u003Cspan style=\"top:-2.357em;margin-left:0em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord text mtight\">\u003Cspan class=\"mord mtight\">new\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.143em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003Cspan class=\"mbin\">+\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.8em;vertical-align:-0.65em;\">\u003C/span>\u003Cspan class=\"minner\">\u003Cspan class=\"mopen delimcenter\" style=\"top:0em;\">\u003Cspan class=\"delimsizing size2\">(\u003C/span>\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.938em;\">\u003Cspan style=\"top:-3.113em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mopen mtight\">(\u003C/span>\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j\u003C/span>\u003Cspan class=\"mclose mtight\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.938em;\">\u003Cspan style=\"top:-3.113em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mopen mtight\">(\u003C/span>\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j\u003C/span>\u003Cspan class=\"mclose mtight\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose delimcenter\" style=\"top:0em;\">\u003Cspan class=\"delimsizing size2\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\n\u003Cp>，最终输出在所有块结束后一次性归一化：\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>O\u003C/mi>\u003Cmo>=\u003C/mo>\u003Cmfrac>\u003Cmrow>\u003Cmi>A\u003C/mi>\u003Cmi>c\u003C/mi>\u003Cmi>c\u003C/mi>\u003C/mrow>\u003Cmi mathvariant=\"normal\">ℓ\u003C/mi>\u003C/mfrac>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">O = \\frac{Acc}{\\ell}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.2173em;vertical-align:-0.345em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mopen nulldelimiter\">\u003C/span>\u003Cspan class=\"mfrac\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8723em;\">\u003Cspan style=\"top:-2.655em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">ℓ\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-3.23em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"frac-line\" style=\"border-bottom-width:0.04em;\">\u003C/span>\u003C/span>\u003Cspan style=\"top:-3.394em;\">\u003Cspan class=\"pstrut\" style=\"height:3em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">A\u003C/span>\u003Cspan class=\"mord mathnormal mtight\">cc\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.345em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose nulldelimiter\">\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>。\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">def\u003C/span>\u003Cspan style=\"color:#B392F0\"> online_softmax_blocked\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(Q_block, K, V, Bc):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">\t\t# Q_block: [Br, d], K: [T, d], V: [T, d]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    Br, d \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> Q_block.shape\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    m \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> np.full((Br,), \u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#E1E4E8\">np.inf, \u003C/span>\u003Cspan style=\"color:#FFAB70\">dtype\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">np.float32) \u003C/span>\u003Cspan style=\"color:#6A737D\"># running max\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    l \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> np.zeros((Br,), \u003C/span>\u003Cspan style=\"color:#FFAB70\">dtype\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">np.float32) \u003C/span>\u003Cspan style=\"color:#6A737D\"># running exp-sum\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    acc \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> np.zeros((Br, d), \u003C/span>\u003Cspan style=\"color:#FFAB70\">dtype\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">np.float32) \u003C/span>\u003Cspan style=\"color:#6A737D\"># running weighted sum\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">\t\tfor\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> start \u003C/span>\u003Cspan style=\"color:#F97583\">in\u003C/span>\u003Cspan style=\"color:#79B8FF\"> range\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, K.shape[\u003C/span>\u003Cspan style=\"color:#79B8FF\">0\u003C/span>\u003Cspan style=\"color:#E1E4E8\">], Bc):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">\t\t    K_blk \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> K[start:start\u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#E1E4E8\">Bc] \u003C/span>\u003Cspan style=\"color:#6A737D\"># [Bc, d]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        V_blk \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> V[start:start\u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#E1E4E8\">Bc] \u003C/span>\u003Cspan style=\"color:#6A737D\"># [Bc, d]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        S \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> (Q_block \u003C/span>\u003Cspan style=\"color:#F97583\">@\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> K_blk.T) \u003C/span>\u003Cspan style=\"color:#F97583\">*\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> scale \u003C/span>\u003Cspan style=\"color:#6A737D\"># [Br, Bc]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        S \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> apply_mask_if_needed(S) \u003C/span>\u003Cspan style=\"color:#6A737D\"># causal/pad -> -inf\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        m_blk \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> S.max(\u003C/span>\u003Cspan style=\"color:#FFAB70\">axis\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) \u003C/span>\u003Cspan style=\"color:#6A737D\"># [Br]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        m_new \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> np.maximum(m, m_blk) \u003C/span>\u003Cspan style=\"color:#6A737D\"># [Br]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">\t\t\t\t# rescale old stats to the new max\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        exp_m \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> np.exp(m \u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> m_new) \u003C/span>\u003Cspan style=\"color:#6A737D\"># [Br]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        l \u003C/span>\u003Cspan style=\"color:#F97583\">*=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> exp_m\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        acc \u003C/span>\u003Cspan style=\"color:#F97583\">*=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> exp_m[:,\u003C/span>\u003Cspan style=\"color:#79B8FF\">None\u003C/span>\u003Cspan style=\"color:#E1E4E8\">]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">\t\t\t\t# add contribution of this block under the new max\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        P \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> np.exp(S \u003C/span>\u003Cspan style=\"color:#F97583\">-\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> m_new[:,\u003C/span>\u003Cspan style=\"color:#79B8FF\">None\u003C/span>\u003Cspan style=\"color:#E1E4E8\">]) \u003C/span>\u003Cspan style=\"color:#6A737D\"># [Br, Bc]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        l \u003C/span>\u003Cspan style=\"color:#F97583\">+=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> P.sum(\u003C/span>\u003Cspan style=\"color:#FFAB70\">axis\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">) \u003C/span>\u003Cspan style=\"color:#6A737D\"># [Br]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        acc \u003C/span>\u003Cspan style=\"color:#F97583\">+=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> P \u003C/span>\u003Cspan style=\"color:#F97583\">@\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> V_blk \u003C/span>\u003Cspan style=\"color:#6A737D\"># [Br, d]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        m \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> m_new\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    O \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> acc \u003C/span>\u003Cspan style=\"color:#F97583\">/\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> l[:,\u003C/span>\u003Cspan style=\"color:#79B8FF\">None\u003C/span>\u003Cspan style=\"color:#E1E4E8\">]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">    LSE\u003C/span>\u003Cspan style=\"color:#F97583\"> =\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> m \u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> np.log(l)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">return\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> O, \u003C/span>\u003Cspan style=\"color:#79B8FF\">LSE\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Chr>\n\u003Ch2 id=\"三flashattention-v2并行化与-kernel-设计改进\">三、FlashAttention V2：并行化与 Kernel 设计改进\u003C/h2>\n\u003Cp>随着模型上下文长度不断攀升（32K 甚至 128K），FlashAttention V1 尽管高效，但仍有进一步优化空间。FlashAttention V2（2023年提出）在保持V1 精确和内存高效特性的基础上，针对 GPU 实现引入了\u003Cstrong>更优的并行策略和工作划分\u003C/strong>，从而实现\u003Cstrong>近2倍\u003C/strong>的加速。其主要改进包括：减少非矩阵计算开销、更好地利用 GPU 并行资源，以及改进线程内的工作分配。\u003C/p>\n\u003Ch3 id=\"31-减少非矩阵运算开销\">\u003Cstrong>3.1 减少非矩阵运算开销\u003C/strong>\u003C/h3>\n\u003Cp>现代 GPU 上的 Tensor Core 对矩阵乘法有极高吞吐，而标量操作（除法、指数、比较等）则慢得多。FlashAttention V2 重新梳理了 V1 中的 Softmax 计算，将一些频繁执行的重缩放（rescale）、边界检查和掩码操作等尽量移出主循环或合并，减少了这些\u003Cstrong>非矩阵类 FLOP\u003C/strong> 的次数。例如，在 Online Softmax 过程中，只在必要时做归一化调整，避免对每块重复做比例缩放。又如将软掩码的检查融入计算流程，尽量利用 GPU 的向量化指令替代显式判断。这些改动虽然不改变算法结果，但节省了很多 GPU 上“慢指令”的开销。据统计，Ampere 架构下一个 FP32 除法/指数等特殊函数的吞吐仅是 Tensor Core 矩阵乘法的 1/16 甚至更低。因此，通过减少这些操作，V2 能让 GPU 更多时间用于高速的矩阵乘法，从而提高整体 FLOPs 利用率。\u003C/p>\n\u003Ch3 id=\"32-跨序列长度的并行\">\u003Cstrong>3.2 跨序列长度的并行\u003C/strong>\u003C/h3>\n\u003Cp>FlashAttention V1 的并行化维度主要是\u003Cstrong>批次（batch）和多头（head）\u003C/strong>。它为每个注意力头启动一个线程块（block），总共启动\u003Ccode>batch_size * num_heads\u003C/code>个线程块并行计算。在典型训练场景（大batch、多头）下，这样可以占满 GPU 的大部分 SM。然而在\u003Cstrong>长序列-小 batch\u003C/strong> 的推理或训练下，V1 常出现 GPU 资源利用不充分的问题：比如 batch size 很小且序列很长时，线程块数不足以占用全部 SM。V2 针对这一情况，增加了沿\u003Cstrong>序列长度方向的并行\u003C/strong>。具体做法是在前向计算中，将注意力矩阵按“查询序列”的行拆分，由多个线程块分别处理不同的行块。这样，即使 batch 很小，每个 head 的长序列也能拆分成多个部分并行计算，从而大幅提高 SM 占用率。如\u003Cstrong>图 3\u003C/strong> 所示，在前向过程中，不同线程块（Worker）各负责注意力矩阵的一部分行；在反向过程中，则各负责一部分列，以避免竞争更新梯度时的冲突。这种沿序列拆分的调度使 FlashAttention 在极长序列、小批量情况下仍能接近满 GPU 并行度，从而\u003Cstrong>支持更长上下文\u003C/strong>并提升此情形下的速度。\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/notion/1fb22dca-4210-80cd-a96e-e32787cfd674/2d022dca-4210-8038-a862-fb77cf289cdf.png\" alt=\"图 3：FlashAttention-2 中跨线程块的并行调度示意。左图为前向传播：每个“Worker”（线程块）负责注意力矩阵的一块行片段，例如 Worker1 处理红色行块，Worker2 处理粉色行块等。右图为反向传播：每个线程块负责一块列片段，例如 Worker1 处理红色列块，Worker2 处理粉色列块等。这种沿序列长度的划分提高了长序列小批量情况下 GPU 的利用率。\">\u003C/p>\n\u003Ch3 id=\"33-改进线程块内的-warp-分工\">\u003Cstrong>3.3 改进线程块内的 Warp 分工\u003C/strong>\u003C/h3>\n\u003Cp>在 GPU 上，一个线程块包含若干个 Warp（每个 Warp 32 个线程）。在 FlashAttention V1 中，采用的是“\u003Cstrong>切分 K（split-K）\u003C/strong>”的方案：即每个 Warp 各自处理一部分 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>K\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">K\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003C/span>\u003C/span>\u003C/span> 和对应的 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span>，计算部分的 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>Q\u003C/mi>\u003Cmsup>\u003Cmi>K\u003C/mi>\u003Cmi>T\u003C/mi>\u003C/msup>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">QK^T\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0358em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8413em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>，然后需要将各 Warp 的结果写入共享内存并同步，加和得到完整输出。这种方案需要 Warps 间频繁同步和共享内存通信，造成一定开销。FlashAttention V2 改为“\u003Cstrong>切分 Q（split-Q）\u003C/strong>”方案：让每个 Warp 处理\u003Cstrong>不同的查询\u003C/strong> \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>Q\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">Q\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003C/span>\u003C/span>\u003C/span> \u003Cstrong>子块\u003C/strong>，而使所有 Warp 都能访问完整的 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>K\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">K\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003C/span>\u003C/span>\u003C/span> 和 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span> 块。这样，每个 Warp 可以独立完成自己那部分 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>Q\u003C/mi>\u003Cmsup>\u003Cmi>K\u003C/mi>\u003Cmi>T\u003C/mi>\u003C/msup>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">QK^T\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0358em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8413em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span> 乘积并直接乘以共享的 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span> 得到对应输出片段，无需与其他 Warp 交换中间结果。\u003Cstrong>图 4\u003C/strong> 展示了两种 Warp 分工方式的对比：FlashAttention-1（左）中不同颜色 Warp 各处理一部分 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>K\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">K,V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span>，需要写共享内存（圆圈部分）再汇总；FlashAttention-2（右）则每个 Warp 处理不同的 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>Q\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">Q\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003C/span>\u003C/span>\u003C/span> 行，避免了 Warp 间通信。这种改进显著减少了片内共享内存的读写和同步屏障，使单个线程块内部执行更高效。\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/notion/1fb22dca-4210-80cd-a96e-e32787cfd674/2d022dca-4210-8014-8fb1-c14621251b02.png\" alt=\"图 4：Warp 级工作划分对比。（左）FlashAttention-1 中采用“切分 K”方案，不同 Warp 处理不同列块（K 块），需要在共享内存同步累加。（右）FlashAttention-2 改为“切分 Q”方案，不同 Warp 处理不同行块（Q 块），各 Warp 直接算出自己负责的输出部分，无需 Warp 间通信。新方案减少了共享内存读写和同步，提升了效率。\">\u003C/p>\n\u003Ch3 id=\"34-支持更大-head-维度与多查询注意力\">\u003Cstrong>3.4 支持更大 Head 维度与多查询注意力\u003C/strong>\u003C/h3>\n\u003Cp>FlashAttention V1 限制每个注意力头维度 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>d\u003C/mi>\u003Cmo>≤\u003C/mo>\u003Cmn>128\u003C/mn>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">d \\le 128\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8304em;vertical-align:-0.136em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">d\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">≤\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6444em;\">\u003C/span>\u003Cspan class=\"mord\">128\u003C/span>\u003C/span>\u003C/span>\u003C/span>，而一些模型使用到 256 维头。V2 通过内存管理优化支持了 \u003Cstrong>head 尺寸扩展到 256\u003C/strong>。此外，V2 新增对多查询注意力（MQA）\u003Cstrong>和\u003C/strong>分组查询注意力（GQA）的支持。这些变体在推理时共享 Key/Value 以减少缓存大小，FlashAttention-2 通过调整索引机制实现了对它们的适配，而无需为每个头单独存储完整的 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>K\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">K,V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span>。\u003C/p>\n\u003Cp>\u003Cstrong>实现与性能：\u003C/strong> FlashAttention-2 基于 NVIDIA 的 CUTLASS 3.x 和 CuTe 库完全重写，实现了高度优化的 CUDA 内核。相较 V1 的定制 CUDA 实现，新版本在寄存器利用和指令级优化上更进一步，并减少了开发复杂度。值得一提的是，社区也有使用 Triton 实现 FlashAttention 的尝试，但 FlashAttention-2 的官方实现性能更胜一筹，可达原 Triton 版本 1.3～2.5 倍速度。综合上述改进，FlashAttention-2 在 A100 GPU 上的前向+后向总速度达到原版的 2 倍左右，FLOPs 利用率提升到理论峰值的 50～73%（约 230 TFLOPs/s，FP16）。在端到端训练 GPT 等模型时，单卡可稳定运行约 225 TFLOPs/s，相当于 72% 的模型 FLOPs 利用率。同时显存占用仍与序列长度线性关系，保持了大幅节省内存的优势。总结来说，FlashAttention-2 \u003Cstrong>通过更佳的并行和更精细的 kernel 设计\u003C/strong>进一步压榨了 GPU 性能，使得即使在极长序列或硬件升级的场景下，注意力层依然高效。\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"四flashattention-v3异步流水线与推理优化\">四、FlashAttention V3：异步流水线与推理优化\u003C/h2>\n\u003Cp>2024 年，FlashAttention 家族又迎来两项重要进展：\u003Cstrong>FlashAttention-3\u003C/strong> 针对最新 H100 等硬件引入\u003Cstrong>异步流水线\u003C/strong>和\u003Cstrong>低精度优化\u003C/strong>，将训练推理速度再次提升 1.5～2 倍。\u003C/p>\n\u003Cp>NVIDIA Hopper 架构（如 H100 GPU）引入了若干新特性：Warp 级并行 GEMM 指令（WGMMA）、Tensor Memory Accelerator（TMA）以及更高效的 FP8 矩阵运算等。FlashAttention-3 利用这些硬件能力，实现了\u003Cstrong>计算与数据传输的重叠并行\u003C/strong>以及\u003Cstrong>更低精度下的高准确率\u003C/strong>，从而在 H100 上将注意力计算的 FLOPs 利用率提高到约 75%（FP16/BF16）甚至 85%（FP8 模式）。\u003C/p>\n\u003Ch3 id=\"41-异步计算与流水线重叠\">\u003Cstrong>4.1 异步计算与流水线重叠\u003C/strong>\u003C/h3>\n\u003Cp>传统注意力计算流程中，矩阵乘法（GEMM）和 Softmax 是串行的：必须先算完所有 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>Q\u003C/mi>\u003Cmsup>\u003Cmi>K\u003C/mi>\u003Cmi>T\u003C/mi>\u003C/msup>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">QK^T\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0358em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8413em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span> 得到注意力得分，再计算 Softmax，再乘 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span>。然而在 H100上，\u003Cstrong>GEMM 和 Softmax 两类操作可以并行重叠\u003C/strong>执行。原因在于它们使用 GPU 上不同的计算单元（Tensor Core vs. 标量单元），例如 H100 的 FP16 Tensor Core 峰值 989 TFLOPs，而执行指数的标量单元仅 ~3.9 TFLOPs，相差达 256 倍。Softmax 尽管 FLOPs 占比不高，但因为速度慢，如果串行执行会占用总时间约一半。理想状况下，我们希望\u003Cstrong>在 Tensor Core 做矩阵乘法的同时，利用其他单元并行计算Softmax\u003C/strong>。\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/notion/1fb22dca-4210-80cd-a96e-e32787cfd674/2d022dca-4210-8058-95fe-d57ccd03d3b6.png\" alt=\"图 5：FlashAttention-3 中跨 Warp 组的 Ping-Pong 异步调度。Warp 组 1（上方）和 Warp 组 2（下方）交替执行矩阵乘 GEMM 和 Softmax 操作：当一组等待矩阵乘结果时，另一组利用空闲执行 Softmax。虚线分隔不同迭代，彩色块表示 GEMM 或 Softmax 所占用的时间段。这样实现两类操作重叠并行，提高了硬件利用率。\">\u003C/p>\n\u003Cp>FlashAttention-3 通过 Warp 级专业化（warp specialization）实现这一点：将同一线程块内的 Warp 分成两个角色，一部分 Warp 专职执行 GEMM（通过新 WGMMA 指令提高吞吐），另一部分 Warp 专职执行 Softmax 和归一化。然后采用“\u003Cstrong>Ping-Pong 调度\u003C/strong>”在 Warp 组间交替执行：例如两个 Warp 组，一组先进行当前块的 GEMM 计算，另一组利用这段时间对上一个块执行 Softmax；随后两组交换角色，如此往复。\u003Cstrong>图 5\u003C/strong> 展示了有两个 Warp 组时的流水线时间表：相同颜色代表同一次迭代，Warp 组 1 先执行 GEMM0 然后 Softmax，而 Warp 组 2 稍后开始 GEMM0，当 Warp 组 1 切换去执行 GEMM1 时，Warp组 2 正好执行 Softmax，以此交错重叠。这种手动的 barrier 调度让两个 Warp 组总有一个在算 Softmax 而另一个在算 GEMM，从而把 Softmax 隐藏在别的计算“阴影”下，提升流水线并行度。实测在 H100 上，FP16 前向算力通过这种 Warp 组 Ping-Pong 调度，速度从 570 TFLOPs 提高到 620 TFLOPs 左右。\u003C/p>\n\u003Cp>\u003Cimg src=\"/images/notion/1fb22dca-4210-80cd-a96e-e32787cfd674/2d022dca-4210-808c-928e-d45bfea290a2.png\" alt=\"图 6\">\u003C/p>\n\u003Cp>除了 Warp 组之间的并行，如图 6 所示，FlashAttention-3 还进一步在\u003Cstrong>单个 Warp 组内实现流水线\u003C/strong>：将一次 Attention 计算拆成两个阶段，先执行部分 GEMM 累积，再插入 Softmax 计算，然后继续下一个 GEMM。通过在 Warp 内交叉执行，小部分 Softmax 计算可以与本组后续的 GEMM 重叠，从而进一步榨取并行度。这种方案增加了一些寄存器开销（需要同时保存 GEMM 累积和 Softmax 中间值），但换来约 3% 左右额外性能提升（FP16 前向从 620 提升到约 640+ TFLOPs）。\u003C/p>\n\u003Ch3 id=\"42-hopper-硬件特性利用\">\u003Cstrong>4.2 Hopper 硬件特性利用\u003C/strong>\u003C/h3>\n\u003Cp>FlashAttention-3 针对 H100 的新指令做了专门优化。例如使用 \u003Cstrong>WGMMA\u003C/strong> 指令批量执行 Warp 组矩阵乘法，提高单 Warp 组算力利用；利用 \u003Cstrong>TMA\u003C/strong> 硬件在后台异步搬运数据块，从 HBM 到共享内存，隐藏内存延迟。通过这些手段，在不增加显式同步的情况下，实现了计算和传输高度重叠。另外，FlashAttention-3 使用 NVIDIA 提供的 CUTLASS 库的抽象封装这些操作，加快开发同时保障性能。这些低级优化共同使得FlashAttention-3 在 H100 上达到了\u003Cstrong>接近峰值\u003C/strong>的性能：FP16/BF16 模式下前向最高约 740 TFLOPs（75% 理论峰值），比 FlashAttention-2 在 A100 上的 124 TFLOPs 大幅提升；使用 FP8 精度时则进一步达到 \u003Cstrong>1.2 PFLOPs\u003C/strong> 的惊人水平。\u003C/p>\n\u003Ch3 id=\"43-低精度-fp8-与数值优化\">\u003Cstrong>4.3 低精度 FP8 与数值优化\u003C/strong>\u003C/h3>\n\u003Cp>除了速度，FlashAttention-3 还探索了\u003Cstrong>更低精度计算\u003C/strong>以提高效率。FP8 精度下 Tensor Core 吞吐可翻倍，但直接将注意力降到 FP8 会引入较大数值误差。模型激活常出现“outlier”离群值，用低位表示会造成严重量化误差。为此，FlashAttention-3 引入了\u003Cstrong>失相干处理（incoherent processing）技术：对\u003C/strong> \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>Q\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>K\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">Q,K\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003C/span>\u003C/span>\u003C/span> \u003Cstrong>每个 head 乘以一个随机正交矩阵（如 Hadamard 变换），将少数大值“扩散”到各维度。这样做可降低量化误差，而且 Hadamard 变换本身是线性操作，可以与其他操作（如 RoPE 位置编码）融合而几乎无额外代价。实验表明，对于模拟含 0.1% 大幅值的输入，失相干处理将 FP8 量化误差降低了2.6倍\u003C/strong>。借助对 outlier 的特殊处理，FlashAttention-3 在 FP8 模式下达到与 FP16 几乎相当的准确率损失：RMSE 误差仅为普通 FP8 实现的约 1/2.6。因此，FlashAttention-3 \u003Cstrong>在不牺牲准确率的前提下\u003C/strong>成功利用 FP8 把速度再次提升约 1.3 倍。\u003C/p>\n\u003Cp>综合来说，FlashAttention-3 针对\u003Cstrong>新硬件的异步并行\u003C/strong>和\u003Cstrong>低精度技巧\u003C/strong>实现了显著性能突破。在 H100 上，其 FP16 推理和训练速度比 FlashAttention-2 提高约 1.5～2 倍，达到 ~75% 理论 FLOPs 利用；FP8 模式下更是达到了近 1.2 PFLOPs 的前所未有速度。这些优化证明：充分发掘硬件并行特性和结合算法创新，仍能在 Transformer 这样成熟的算子上取得大幅改进。这也为未来进一步优化 LLM 推理、以及在其它硬件上移植类似技术指明了方向。\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"五flash-decoding自回归长序列解码的并行优化\">五、Flash-Decoding：自回归长序列解码的并行优化\u003C/h2>\n\u003Cp>上述 FlashAttention 系列主要针对\u003Cstrong>训练\u003C/strong>场景的长序列优化。然而在\u003Cstrong>推理/解码\u003C/strong>阶段，Transformer 通常是\u003Cstrong>自回归\u003C/strong>地一个一个地生成 token，每次只计算一步注意力。典型情况下，解码时当前查询长度为 1（即每次只生成一个新 token），但需要和前面可能数千甚至数万长度的上下文计算注意力。这一特点导致原版 FlashAttention 在推理时遇到新的性能挑战：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>低并行度问题：\u003C/strong> 自回归解码时每次只有一个查询向量，FlashAttention V1/V2 将线程块并行在查询长度和 batch 上。如果 batch size 也很小（常见于单句生成），那么 GPU 的大部分 SM 在计算一个注意力头时都是闲置的（例如 A100 有 108 个SM，而 batch=1 时 FlashAttention 只用到 1 个 SM 不到）。即使序列很长，\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>K\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">K,V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span> 很多，V1/V2 也只能在一两个 SM 上顺序分块处理，\u003Cstrong>GPU 利用率极低\u003C/strong>。\u003C/li>\n\u003Cli>\u003Cstrong>注意力仍是主要瓶颈：\u003C/strong> 在推理阶段，Transformer 的其余计算（前馈网络等）可以缓存和批处理，但\u003Cstrong>注意力的计算量随上下文长度增长\u003C/strong>。特别是当支持超长上下文（数万 token）时，注意力占据生成过程的大部分时间。为提升吞吐，必须针对这种“一对多”的特殊情况优化注意力 kernel。\u003C/li>\n\u003C/ul>\n\u003Cp>为此，Tri Dao 等人提出了 \u003Cstrong>Flash-Decoding\u003C/strong> 技术。它借鉴 FlashAttention 的思路，但新增\u003Cstrong>沿 KV 长度的并行\u003C/strong>来充分利用 GPU，即\u003Cstrong>对键/值序列进行拆分并行处理\u003C/strong>。核心思想如下：\u003C/p>\n\u003Col>\n\u003Cli>\u003Cstrong>切分 KV 缓存：\u003Cstrong>将全部过往 Tokens 的键、值 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>K\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">K, V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span> 矩阵按序列长度方向分割成若干较小的\u003C/strong>块（chunk）\u003C/strong>。例如总长度 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>N\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">N\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N\u003C/span>\u003C/span>\u003C/span>\u003C/span> 分成 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>M\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">M\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M\u003C/span>\u003C/span>\u003C/span>\u003C/span> 个 chunk，每个大小约 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>N\u003C/mi>\u003Cmi mathvariant=\"normal\">/\u003C/mi>\u003Cmi>M\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">N/M\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N\u003C/span>\u003Cspan class=\"mord\">/\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M\u003C/span>\u003C/span>\u003C/span>\u003C/span>。这些块仅是对原 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>K\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">K,V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span> 在内存中的视图，不需要真实拷贝。\u003C/li>\n\u003Cli>\u003Cstrong>并行计算局部注意力：\u003Cstrong>为每个 chunk 启动一个 FlashAttention 内核，计算\u003C/strong>当前查询与该 chunk\u003C/strong> 的注意力输出\u003Cstrong>部分\u003C/strong> \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsup>\u003Cmi>O\u003C/mi>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>j\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003C/msup>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">O^{(j)}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.888em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.888em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mopen mtight\">(\u003C/span>\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j\u003C/span>\u003Cspan class=\"mclose mtight\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>，同时计算该 chunk 局部的 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmtext>log-sum-exp\u003C/mtext>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\text{log-sum-exp}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord text\">\u003Cspan class=\"mord\">log-sum-exp\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span> 值（对应 Softmax 分母的一部分）并存储。这一步相当于并行执行多次“查询与子序列”的注意力，产生各自归一化的局部结果和一个缩放系数。\u003C/li>\n\u003Cli>\u003Cstrong>跨块归并输出：\u003Cstrong>在上述并行计算完成后，再启动一个小 kernel 将各 chunk 的部分输出 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsup>\u003Cmi>O\u003C/mi>\u003Cmrow>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>j\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003C/msup>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">O^{(j)}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.888em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.888em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mopen mtight\">(\u003C/span>\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j\u003C/span>\u003Cspan class=\"mclose mtight\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span> 合并成完整输出 。合并时利用每个 chunk 提供的 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>log\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003Cmtext>-sum-exp\u003C/mtext>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\log\\text{-sum-exp}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8889em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mop\">lo\u003Cspan style=\"margin-right:0.01389em;\">g\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord text\">\u003Cspan class=\"mord\">-sum-exp\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span> 信息，按概率正确加权叠加各部分。这相当于执行一次\u003C/strong>全局 Softmax 归一化\u003C/strong>：把各块之前局部 Softmax 得到的值按照它们占全局分母的比例进行缩放和累加。\u003C/li>\n\u003C/ol>\n\u003Cp>通过上述过程，Flash-Decoding 实现了\u003Cstrong>两级在线 Softmax\u003C/strong>：每个 chunk 内部用了 FlashAttention 的在线算法计算局部 Softmax，chunk 之间再通过一次归并完成全局 Softmax。关键在于，第二步的多 kernel 完全并行使得 GPU 所有 SM 都被利用来处理不同段的 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>K\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>V\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">K,V\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V\u003C/span>\u003C/span>\u003C/span>\u003C/span>。只要上下文长度足够大划分出足够 chunk，即使 batch=1，GPU 也可以\u003Cstrong>满载运行\u003C/strong>注意力计算。这使得推理时注意力耗时基本只随显存带宽线性增长，而不像以前随序列长度急剧恶化。\u003C/p>\n\u003Cp>实验显示，在如 Code Llama-34B 等模型上，Flash-Decoding 对长序列（比如 64k tokens）推理速度有\u003Cstrong>量级提升\u003C/strong>：与标准 PyTorch 或 FasterTransformer 等方案相比，长上下文下吞吐最高提升可达 \u003Cstrong>8 倍\u003C/strong>。并且它几乎实现了“横向扩展”：序列长度从 512 增加到 64k，Flash-Decoding 方案的生成速度几乎不受影响，而传统注意力方法速度随长度显著下降。值得强调的是，这种加速\u003Cstrong>完全不影响输出结果\u003C/strong>，仍然是精确的注意力计算，只是巧妙利用了并行资源。Flash-Decoding 已集成进 FlashAttention 库的推理接口（v2.2 版起），为长上下文模型的实际部署提供了实用方案。\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"六flashdecoding进一步降低生成延迟的优化\">六、FlashDecoding++：进一步降低生成延迟的优化\u003C/h2>\n\u003Cp>在 Flash-Decoding 基础上，学界又提出了 \u003Cstrong>FlashDecoding++\u003C/strong>（MLSys 2024），该工作由清华大学等团队完成，着重优化了\u003Cstrong>推理延迟和跨硬件适配\u003C/strong>。FlashDecoding++ 在保持 Flash-Decoding 并行框架的同时，引入三项新技术：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>异步 Softmax 管线（Asynchronized Softmax）与统一最大值：\u003C/strong> 针对 Flash-Decoding 归并各 chunk Softmax 时需要同步等待的问题，FlashDecoding++ 提出\u003Cstrong>统一最大值\u003C/strong>技巧，消除不同部分 Softmax 在归一化时的同步依赖。通过在各部分 Softmax 时引用同一个全局最大值来调整，Softmax 计算可以更早进行流水，减少阻塞。配合\u003Cstrong>细粒度流水线\u003C/strong>调度，作者报告在预填充阶段加速1.18×，解码阶段加速1.14×。\u003C/li>\n\u003Cli>\u003Cstrong>平坦 GEMM 优化与双缓冲：\u003C/strong> 生成过程中，小批量时会出现很多不同形状的小矩阵乘（如维度不匹配的 \u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>Q\u003C/mi>\u003Cmo>∗\u003C/mo>\u003Cmi>K\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">Q*K\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">Q\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003Cspan class=\"mbin\">∗\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6833em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K\u003C/span>\u003C/span>\u003C/span>\u003C/span>、以及一列一列增大的投影矩阵等），这些\u003Cstrong>非均匀 GEMM\u003C/strong> 难以被统一优化。FlashDecoding++ 分析了这些 GEMM 瓶颈，引入双缓冲（double buffering）等技术，使得 GPU 在执行一个 GEMM 时预取下一个，隐藏内存延迟。针对某些“扁平”大矩阵乘（flat GEMM）的特殊优化带来了高达 52% 的 GEMM 加速。\u003C/li>\n\u003Cli>\u003Cstrong>启发式数据流调度：\u003C/strong> 考虑不同硬件（如 NVIDIA Tensor Core vs AMD 矩阵核心）和不同输入特征下，\u003Cstrong>静态统一的数据流\u003C/strong>未必最优。FlashDecoding++ 通过\u003Cstrong>启发式策略\u003C/strong>自适应选择计算走向：例如对小矩阵选用标量核心执行、对大矩阵用 Tensor Core，或对不同 batch /长度采用不同并行度。这种\u003Cstrong>硬件资源自适应\u003C/strong>的数据流让各种场景下都接近最优，作者报告相比固定策略最多提升 29% 速度。\u003C/li>\n\u003C/ul>\n\u003Cp>综合以上优化，FlashDecoding++ 展现出\u003Cstrong>显著的端到端性能提升\u003C/strong>：在 NVIDIA A100 和 AMD MI210 上对主流 LLM 模型推理实现了\u003Cstrong>平均 1.37× 速度提升。尤其在首token延迟\u003C/strong>和\u003Cstrong>流式生成每个 token 延迟\u003C/strong>上，FlashDecoding++ 均领先。\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"七总结flashattention-演进路径与应用场景\">七、总结：FlashAttention 演进路径与应用场景\u003C/h2>\n\u003Cp>从 FlashAttention V1 到 V3，以及针对推理的 Flash-Decoding 系列，注意力加速技术完成了从 \u003Cstrong>IO 优化\u003C/strong>到\u003Cstrong>并行极限\u003C/strong>、从\u003Cstrong>通用训练\u003C/strong>到\u003Cstrong>专门推理\u003C/strong>的逐步演进：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>FlashAttention V1\u003C/strong> 聚焦于\u003Cstrong>算法重组和内存层次优化\u003C/strong>，通过块式计算和在线 Softmax 使注意力计算在 GPU 上更加“IO 友好”，实现了注意力精确计算的首次大幅加速。它适用于几乎所有 Transformer 注意力计算场景，大幅降低了显存占用并提升训练速度，已经成为众多开源模型的默认实现。\u003C/li>\n\u003Cli>\u003Cstrong>FlashAttention V2\u003C/strong> 在 V1 基础上深入 \u003Cstrong>GPU 并行架构\u003C/strong>优化，解决了长序列小批量时的扩展性问题，引入跨序列并行和 warp 级无同步分工，实现了接近硬件上限的效率。它使得\u003Cstrong>超长上下文训练\u003C/strong>成为可能（例如 2 倍序列长度下仍可高效训练），并在 A100 上达到了与 GEMM 相当级别的高利用。V2 适用于训练和推理中\u003Cstrong>序列长度极长\u003C/strong>但需要充分利用硬件的情况。\u003C/li>\n\u003Cli>\u003Cstrong>FlashAttention V3\u003C/strong> 利用\u003Cstrong>新硬件特性\u003C/strong>（如 H100 的异步 Tensor Core、FP8）将注意力计算推进到\u003Cstrong>流水线并行的新阶段\u003C/strong>，显著提升了 Hopper GPU 上的性能上限。通过异步重叠计算和低精度优化，V3 为\u003Cstrong>下一代硬件\u003C/strong>上的 LLM 训练和推理奠定了基础。其 Warp 级流水线思想也可为其他 GPU 程序优化提供思路。\u003C/li>\n\u003Cli>\u003Cstrong>Flash-Decoding 系列\u003C/strong>专门面向\u003Cstrong>自回归推理\u003C/strong>这一新挑战，通过\u003Cstrong>并行化 attention 计算\u003C/strong>和一系列 pipeline 改进，将长上下文生成的\u003Cstrong>端到端延迟显著降低\u003C/strong>。Flash-Decoding 让长度从几千提升到数万时的推理成本不再高不可攀，对于需要长文档处理、对话持续上下文等应用非常关键。而FlashDecoding++ 进一步优化了\u003Cstrong>单步延迟\u003C/strong>和\u003Cstrong>跨硬件性能\u003C/strong>，在工业部署中具有吸引力。\u003C/li>\n\u003C/ul>\n\u003Cp>总之，FlashAttention 从 V1 到 V3，以及针对解码的扩展，一步步攻克了 Transformer 注意力在\u003Cstrong>存储、并行、延迟\u003C/strong>上的瓶颈，成为 Transformer 加速领域的重要里程碑。对于工程实践者来说，这些算法既提供了\u003Cstrong>开箱即用\u003C/strong>的性能提升，也展现了\u003Cstrong>贴合硬件特性\u003C/strong>进行深度优化的范例。在未来，我们有理由期待这些思路进一步推广到更多模型组件和硬件平台，继续突破大模型训练与推理的效率极限。\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"参考链接\">参考链接\u003C/h2>\n\u003Cul>\n\u003Cli>\n\u003Cp>From Online Softmax to FlashAttention\u003C/p>\n\u003Cp>\u003Ca href=\"https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf\">https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf\u003C/a>\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Online Softmax to Flash Attention — and Why it Matters | by Matthew Gunton | Data Science Collective | Medium\u003C/p>\n\u003Cp>\u003Ca href=\"https://medium.com/data-science-collective/online-softmax-to-flash-attention-and-why-it-matters-9d676e7c50a8\">https://medium.com/data-science-collective/online-softmax-to-flash-attention-and-why-it-matters-9d676e7c50a8\u003C/a>\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>FlashAttention 2: making Transformers 800% faster w/o approximation - with Tri Dao of Together AI\u003C/p>\n\u003Cp>\u003Ca href=\"https://www.latent.space/p/flashattention\">https://www.latent.space/p/flashattention\u003C/a>\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>FlashAttention by hand - DEV Community\u003C/p>\n\u003Cp>\u003Ca href=\"https://dev.to/lewis_won/flashattention-by-hand-34im\">https://dev.to/lewis_won/flashattention-by-hand-34im\u003C/a>\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>Aman’s AI Journal • Primers • FlashAttention\u003C/p>\n\u003Cp>\u003Ca href=\"https://aman.ai/primers/ai/flashattention/\">https://aman.ai/primers/ai/flashattention/\u003C/a>\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision | Tri Dao\u003C/p>\n\u003Cp>\u003Ca href=\"https://tridao.me/blog/2024/flash3/\">https://tridao.me/blog/2024/flash3/\u003C/a>\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Flash-Decoding for long-context inference\u003C/strong>\u003C/p>\n\u003Cp>\u003Ca href=\"https://crfm.stanford.edu/2023/10/12/flashdecoding.html\">https://crfm.stanford.edu/2023/10/12/flashdecoding.html\u003C/a>\u003C/p>\n\u003C/li>\n\u003Cli>\n\u003Cp>FlashDecoding++: Faster Large Language Model Inference with Asynchronization, Flat GEMM Optimization, and Heuristics\u003C/p>\n\u003Cp>\u003Ca href=\"https://proceedings.mlsys.org/paper_files/paper/2024/file/5321b1dabcd2be188d796c21b733e8c7-Paper-Conference.pdf\">https://proceedings.mlsys.org/paper_files/paper/2024/file/5321b1dabcd2be188d796c21b733e8c7-Paper-Conference.pdf\u003C/a>\u003C/p>\n\u003C/li>\n\u003C/ul>",{"headings":89,"localImagePaths":145,"remoteImagePaths":146,"frontmatter":147,"imagePaths":150},[90,92,95,98,101,104,107,110,113,116,119,122,125,128,131,134,137,140,143],{"depth":28,"slug":91,"text":91},"学习链接",{"depth":28,"slug":93,"text":94},"一引言attention-机制与内存瓶颈背景","一、引言：Attention 机制与内存瓶颈背景",{"depth":28,"slug":96,"text":97},"二flashattention-v1块式算法与-online-softmax-优化","二、FlashAttention V1：块式算法与 Online Softmax 优化",{"depth":38,"slug":99,"text":100},"21-块式计算与内存优化","2.1 块式计算与内存优化",{"depth":38,"slug":102,"text":103},"22-online-softmax把全行归一化拆成逐块可合并的稳定统计量","2.2 Online Softmax：把“全行归一化”拆成“逐块可合并”的稳定统计量",{"depth":38,"slug":105,"text":106},"23-累积输出把-esvsum-esvesv-也做成可合并的形式","2.3 累积输出：把 ∑eSV\\sum e^${S}V∑eSV 也做成“可合并”的形式",{"depth":28,"slug":108,"text":109},"三flashattention-v2并行化与-kernel-设计改进","三、FlashAttention V2：并行化与 Kernel 设计改进",{"depth":38,"slug":111,"text":112},"31-减少非矩阵运算开销","3.1 减少非矩阵运算开销",{"depth":38,"slug":114,"text":115},"32-跨序列长度的并行","3.2 跨序列长度的并行",{"depth":38,"slug":117,"text":118},"33-改进线程块内的-warp-分工","3.3 改进线程块内的 Warp 分工",{"depth":38,"slug":120,"text":121},"34-支持更大-head-维度与多查询注意力","3.4 支持更大 Head 维度与多查询注意力",{"depth":28,"slug":123,"text":124},"四flashattention-v3异步流水线与推理优化","四、FlashAttention V3：异步流水线与推理优化",{"depth":38,"slug":126,"text":127},"41-异步计算与流水线重叠","4.1 异步计算与流水线重叠",{"depth":38,"slug":129,"text":130},"42-hopper-硬件特性利用","4.2 Hopper 硬件特性利用",{"depth":38,"slug":132,"text":133},"43-低精度-fp8-与数值优化","4.3 低精度 FP8 与数值优化",{"depth":28,"slug":135,"text":136},"五flash-decoding自回归长序列解码的并行优化","五、Flash-Decoding：自回归长序列解码的并行优化",{"depth":28,"slug":138,"text":139},"六flashdecoding进一步降低生成延迟的优化","六、FlashDecoding++：进一步降低生成延迟的优化",{"depth":28,"slug":141,"text":142},"七总结flashattention-演进路径与应用场景","七、总结：FlashAttention 演进路径与应用场景",{"depth":28,"slug":144,"text":144},"参考链接",[],[],{"title":79,"slug":76,"date":71,"tags":148,"status":18,"excerpt":17,"cover":17,"notionId":82,"lastEditedTime":149},[],"2025-12-25T17:12:00.000Z",[],"notion/flashattention.md"]