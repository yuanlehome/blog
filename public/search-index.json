[
  {
    "title": "Code is not only an implementation, but also a presentation of a way of thinking",
    "slug": "code-is-not-only-an-implementation-but-also-a-presentation-of-a-way-of-thinking",
    "date": "2025-12-25",
    "tags": [],
    "excerpt": " <u>_**在日常开发中，我们往往把重点放在“把功能实现出来”。但一个成熟工程师的代码，并不仅仅是代码本身，而是其思维方式、问题拆解能力、沟通意识与系统理解的外化形式。**_</u> ## 一、系统思维：看到代码之外的系统 优秀工程师写代码时并不是只盯着一个函数或一个类，而是思考： 1. 这个模块在整个系统中承担什么角色？ 2. 它如何与其他模块协作？ 3. 当模块演化时，哪些边界会受到影响？ 4. 未来扩展时，能否复用当前设计？可能的瓶颈是什么？ <u>_具备系统视角的代码更加稳健，也更容易在团队内被复用，不会"
  },
  {
    "title": "Context Parallel 技术解析",
    "slug": "context-parallel",
    "date": "2025-12-25",
    "tags": [],
    "excerpt": " --- ## 学习链接 - [[并行训练]Context Parallelism的原理与代码浅析 - 知乎](https://zhuanlan.zhihu.com/p/698447429?share_code=WwUutv3avJIE&utm_psn=1983881801186444490) - [ring attention + flash attention：超长上下文之路 - 知乎](https://zhuanlan.zhihu.com/p/683714620) - [大模型训练之序列并行双雄：DeepSp"
  },
  {
    "title": "DeepGEMM 学习指南：面向初学者的 FP8 GEMM 库解析",
    "slug": "deepgemm-fp8-gemm",
    "date": "2025-12-25",
    "tags": [],
    "excerpt": " --- ## 优质博客 - CUTLASS & GPU CUDA 编程解析 [https://research.colfax-intl.com/blog/](https://research.colfax-intl.com/blog/) ## 一、总体设计与原理 ### 1.1 DeepGEMM 是什么？ DeepGEMM 是由 DeepSeek 团队开源的一个专注于 FP8 矩阵乘法（GEMM）的高效库。它旨在为深度学习中的矩阵乘法提供**高效**且**简洁**的实现，同时支持常规密集矩阵乘法和混合专家模型（M"
  },
  {
    "title": "FlashAttention 原理与实现",
    "slug": "flashattention",
    "date": "2025-12-25",
    "tags": [],
    "excerpt": " --- ## 学习链接 - [图解大模型计算加速系列：FlashAttention V1，从硬件到计算逻辑 - 知乎](https://zhuanlan.zhihu.com/p/669926191) - [图解大模型计算加速系列：FlashAttention V2，从原理到并行计算 - 知乎](https://zhuanlan.zhihu.com/p/691067658) - [\\[Attention优化\\]\\[2w字\\]📚原理篇: 从Online-Softmax到FlashAttention V1/V2/V3"
  },
  {
    "title": "Long Context 推理优化技术梳理",
    "slug": "long-context",
    "date": "2025-12-25",
    "tags": [],
    "excerpt": " --- ## 优质博客 - [[LLM性能优化] 聊聊长文本推理性能优化方向 - 知乎](https://zhuanlan.zhihu.com/p/698308542) - [Deploying DeepSeek with PD Disaggregation and Large-Scale Expert Parallelism on 96 H100 GPUs | LMSYS Org](https://lmsys.org/blog/2025-05-05-large-scale-ep/) - [Implement F"
  },
  {
    "title": "RDMA 在大模型推理框架中的应用",
    "slug": "rdma",
    "date": "2025-12-25",
    "tags": [],
    "excerpt": " ## [极简] RDMA 在大模型推理框架中的应用 --- 在现代大规模 LLM 推理系统中，**多 GPU / 多节点推理已成为必然**。无论是模型本身的体积（70B/405B/1T+）还是长上下文（128k~1M tokens），都远超单节点能力。 为保证吞吐和延迟，推理框架需要极为高效的跨 GPU / 跨节点通信，而 **RDMA（Remote Direct Memory Access）** 由于具备： - **GPU → GPU 的零拷贝（GPUDirect RDMA）** - **绕过 CPU 和内核协"
  },
  {
    "title": "RoPE 究竟是怎么计算的",
    "slug": "rope",
    "date": "2025-12-25",
    "tags": [],
    "excerpt": " --- ## 一、RoPE 到底改了什么？ Attention 里我们原来用： - $Q, K, V$ 计算 $\\text{Attn}(Q, K, V)$ RoPE 做的是把 **Q/K 先旋转**： - $Q' = \\text{RoPE}(Q, \\text{pos})$ - $K' = \\text{RoPE}(K, \\text{pos})$ - 然后用 $Q', K', V$ 做 attention --- ## 二、数学形式：2D 旋转（每两维一组） 对每个 position $p$，对某一对维度 $(u,v"
  },
  {
    "title": "一种 TP-SP-EP 混合并行策略",
    "slug": "tp-sp-ep",
    "date": "2025-12-25",
    "tags": [],
    "excerpt": " --- ## 一、两种混合并行图示 ![非完整图示，不含后续 EP 并行 MoE 层。](/images/notion/2a122dca-4210-805b-ae7e-fb6b09a2e44f/2b222dca-4210-80d9-98fb-cf78ef53eb91.jpeg) --- ## 二、并行原理解析 ### 2.1 前提：`qkv_inear` (列切) 两种方案都始于一个**列并行 (Column-Parallel)** 的 `qkv_inear` 层。 - 我们有 $N$ 个 GPU。 - 输入 $"
  },
  {
    "title": "Understanding Conway’s Law（康威定律）",
    "slug": "understanding-conways-law",
    "date": "2025-12-25",
    "tags": [],
    "excerpt": " <u>_**康威定律（Conway’s Law）：设计系统的组织，其产生的系统设计，必然反映该组织的沟通结构。**_</u> 软件代码结构是部门组织形态的体现，分支管理则是这种组织形态在时间维度上的投影。组织如何沟通、如何划分责任、如何整合成果，都会直接在代码仓库的结构与分支策略中体现。优秀的组织会通过良好的分支策略促进协作，糟糕的分支策略则会放大组织壁垒。 ### 一、软件结构与组织形态的镜像关系 软件代码结构是部门组织形态的体现。换句话说，组织如何划分团队、沟通与协作，最终都会被固化在系统架构与代码模块中。 "
  }
]