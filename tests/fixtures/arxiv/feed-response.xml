<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query=&amp;id_list=2306.00978&amp;start=0&amp;max_results=10" rel="self" type="application/atom+xml"/>
  <title type="html">arXiv Query: search_query=&amp;id_list=2306.00978&amp;start=0&amp;max_results=10</title>
  <id>http://arxiv.org/api/cHxbiOdZaP56ODnBPIenZhzg5f8</id>
  <updated>2023-07-01T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">10</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2306.00978v5</id>
    <updated>2024-01-15T08:23:45Z</updated>
    <published>2023-06-01T17:59:59Z</published>
    <title>Evaluating Large Language Models at Evaluating Instruction Following</title>
    <summary>We study the use of large language models (LLMs) for evaluating the
ability of models to follow natural language instructions. This paper presents
a comprehensive analysis of instruction-following evaluation methods.</summary>
    <author>
      <name>Zhiqing Sun</name>
    </author>
    <author>
      <name>Yiming Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.00978v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.00978v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
